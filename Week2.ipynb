{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import recall_score, precision_score # New\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.preprocessing\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Add, Multiply, Subtract\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Activation, BatchNormalization\n",
    "# regularizers\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Dropout, SpatialDropout1D\n",
    "from scipy import signal\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'C:/Users/smartgrid_AI/Documents/kms/Python_Study/3. Panama.csv'\n",
    "dat_source = pd.read_csv(data_path)\n",
    "dat_source = pd.DataFrame(dat_source)\n",
    "#dat_s = np.array(dat_source, dtype='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>nat_demand</th>\n",
       "      <th>T2M_toc</th>\n",
       "      <th>QV2M_toc</th>\n",
       "      <th>TQL_toc</th>\n",
       "      <th>W2M_toc</th>\n",
       "      <th>T2M_san</th>\n",
       "      <th>QV2M_san</th>\n",
       "      <th>TQL_san</th>\n",
       "      <th>W2M_san</th>\n",
       "      <th>T2M_dav</th>\n",
       "      <th>QV2M_dav</th>\n",
       "      <th>TQL_dav</th>\n",
       "      <th>W2M_dav</th>\n",
       "      <th>Holiday_ID</th>\n",
       "      <th>holiday</th>\n",
       "      <th>school</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>03-01-2015 01:00</td>\n",
       "      <td>970.3450</td>\n",
       "      <td>25.865259</td>\n",
       "      <td>0.018576</td>\n",
       "      <td>0.016174</td>\n",
       "      <td>21.850546</td>\n",
       "      <td>23.482446</td>\n",
       "      <td>0.017272</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>10.328949</td>\n",
       "      <td>22.662134</td>\n",
       "      <td>0.016562</td>\n",
       "      <td>0.096100</td>\n",
       "      <td>5.364148</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03-01-2015 02:00</td>\n",
       "      <td>912.1755</td>\n",
       "      <td>25.899255</td>\n",
       "      <td>0.018653</td>\n",
       "      <td>0.016418</td>\n",
       "      <td>22.166944</td>\n",
       "      <td>23.399255</td>\n",
       "      <td>0.017265</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>10.681517</td>\n",
       "      <td>22.578943</td>\n",
       "      <td>0.016509</td>\n",
       "      <td>0.087646</td>\n",
       "      <td>5.572471</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03-01-2015 03:00</td>\n",
       "      <td>900.2688</td>\n",
       "      <td>25.937280</td>\n",
       "      <td>0.018768</td>\n",
       "      <td>0.015480</td>\n",
       "      <td>22.454911</td>\n",
       "      <td>23.343530</td>\n",
       "      <td>0.017211</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>10.874924</td>\n",
       "      <td>22.531030</td>\n",
       "      <td>0.016479</td>\n",
       "      <td>0.078735</td>\n",
       "      <td>5.871184</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>03-01-2015 04:00</td>\n",
       "      <td>889.9538</td>\n",
       "      <td>25.957544</td>\n",
       "      <td>0.018890</td>\n",
       "      <td>0.016273</td>\n",
       "      <td>22.110481</td>\n",
       "      <td>23.238794</td>\n",
       "      <td>0.017128</td>\n",
       "      <td>0.002599</td>\n",
       "      <td>10.518620</td>\n",
       "      <td>22.512231</td>\n",
       "      <td>0.016487</td>\n",
       "      <td>0.068390</td>\n",
       "      <td>5.883621</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>03-01-2015 05:00</td>\n",
       "      <td>893.6865</td>\n",
       "      <td>25.973840</td>\n",
       "      <td>0.018981</td>\n",
       "      <td>0.017281</td>\n",
       "      <td>21.186089</td>\n",
       "      <td>23.075403</td>\n",
       "      <td>0.017059</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>9.733589</td>\n",
       "      <td>22.481653</td>\n",
       "      <td>0.016456</td>\n",
       "      <td>0.064362</td>\n",
       "      <td>5.611724</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43770</th>\n",
       "      <td>31-12-2019 19:00</td>\n",
       "      <td>1301.6065</td>\n",
       "      <td>26.635645</td>\n",
       "      <td>0.018421</td>\n",
       "      <td>0.013165</td>\n",
       "      <td>13.184052</td>\n",
       "      <td>25.135645</td>\n",
       "      <td>0.018048</td>\n",
       "      <td>0.064240</td>\n",
       "      <td>3.086798</td>\n",
       "      <td>23.620020</td>\n",
       "      <td>0.016697</td>\n",
       "      <td>0.073425</td>\n",
       "      <td>3.865351</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43771</th>\n",
       "      <td>31-12-2019 20:00</td>\n",
       "      <td>1250.9634</td>\n",
       "      <td>26.495935</td>\n",
       "      <td>0.018162</td>\n",
       "      <td>0.014713</td>\n",
       "      <td>13.443892</td>\n",
       "      <td>24.769373</td>\n",
       "      <td>0.017781</td>\n",
       "      <td>0.058838</td>\n",
       "      <td>3.659980</td>\n",
       "      <td>23.284998</td>\n",
       "      <td>0.016606</td>\n",
       "      <td>0.064362</td>\n",
       "      <td>4.171572</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43772</th>\n",
       "      <td>31-12-2019 21:00</td>\n",
       "      <td>1193.6802</td>\n",
       "      <td>26.354456</td>\n",
       "      <td>0.017980</td>\n",
       "      <td>0.013836</td>\n",
       "      <td>13.442195</td>\n",
       "      <td>24.479456</td>\n",
       "      <td>0.017606</td>\n",
       "      <td>0.038086</td>\n",
       "      <td>3.769294</td>\n",
       "      <td>23.041956</td>\n",
       "      <td>0.016492</td>\n",
       "      <td>0.054260</td>\n",
       "      <td>4.045283</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43773</th>\n",
       "      <td>31-12-2019 22:00</td>\n",
       "      <td>1130.4575</td>\n",
       "      <td>26.166895</td>\n",
       "      <td>0.017965</td>\n",
       "      <td>0.018486</td>\n",
       "      <td>13.420656</td>\n",
       "      <td>24.112207</td>\n",
       "      <td>0.017393</td>\n",
       "      <td>0.020386</td>\n",
       "      <td>3.872397</td>\n",
       "      <td>22.862207</td>\n",
       "      <td>0.016401</td>\n",
       "      <td>0.055557</td>\n",
       "      <td>3.843736</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43774</th>\n",
       "      <td>31-12-2019 23:00</td>\n",
       "      <td>1084.4737</td>\n",
       "      <td>25.976373</td>\n",
       "      <td>0.018072</td>\n",
       "      <td>0.023315</td>\n",
       "      <td>13.749788</td>\n",
       "      <td>23.663873</td>\n",
       "      <td>0.017156</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>4.165276</td>\n",
       "      <td>22.726373</td>\n",
       "      <td>0.016302</td>\n",
       "      <td>0.061371</td>\n",
       "      <td>3.793209</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43775 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               datetime  nat_demand    T2M_toc  QV2M_toc   TQL_toc    W2M_toc  \\\n",
       "0      03-01-2015 01:00    970.3450  25.865259  0.018576  0.016174  21.850546   \n",
       "1      03-01-2015 02:00    912.1755  25.899255  0.018653  0.016418  22.166944   \n",
       "2      03-01-2015 03:00    900.2688  25.937280  0.018768  0.015480  22.454911   \n",
       "3      03-01-2015 04:00    889.9538  25.957544  0.018890  0.016273  22.110481   \n",
       "4      03-01-2015 05:00    893.6865  25.973840  0.018981  0.017281  21.186089   \n",
       "...                 ...         ...        ...       ...       ...        ...   \n",
       "43770  31-12-2019 19:00   1301.6065  26.635645  0.018421  0.013165  13.184052   \n",
       "43771  31-12-2019 20:00   1250.9634  26.495935  0.018162  0.014713  13.443892   \n",
       "43772  31-12-2019 21:00   1193.6802  26.354456  0.017980  0.013836  13.442195   \n",
       "43773  31-12-2019 22:00   1130.4575  26.166895  0.017965  0.018486  13.420656   \n",
       "43774  31-12-2019 23:00   1084.4737  25.976373  0.018072  0.023315  13.749788   \n",
       "\n",
       "         T2M_san  QV2M_san   TQL_san    W2M_san    T2M_dav  QV2M_dav  \\\n",
       "0      23.482446  0.017272  0.001855  10.328949  22.662134  0.016562   \n",
       "1      23.399255  0.017265  0.001327  10.681517  22.578943  0.016509   \n",
       "2      23.343530  0.017211  0.001428  10.874924  22.531030  0.016479   \n",
       "3      23.238794  0.017128  0.002599  10.518620  22.512231  0.016487   \n",
       "4      23.075403  0.017059  0.001729   9.733589  22.481653  0.016456   \n",
       "...          ...       ...       ...        ...        ...       ...   \n",
       "43770  25.135645  0.018048  0.064240   3.086798  23.620020  0.016697   \n",
       "43771  24.769373  0.017781  0.058838   3.659980  23.284998  0.016606   \n",
       "43772  24.479456  0.017606  0.038086   3.769294  23.041956  0.016492   \n",
       "43773  24.112207  0.017393  0.020386   3.872397  22.862207  0.016401   \n",
       "43774  23.663873  0.017156  0.019531   4.165276  22.726373  0.016302   \n",
       "\n",
       "        TQL_dav   W2M_dav  Holiday_ID  holiday  school  \n",
       "0      0.096100  5.364148           0        0       0  \n",
       "1      0.087646  5.572471           0        0       0  \n",
       "2      0.078735  5.871184           0        0       0  \n",
       "3      0.068390  5.883621           0        0       0  \n",
       "4      0.064362  5.611724           0        0       0  \n",
       "...         ...       ...         ...      ...     ...  \n",
       "43770  0.073425  3.865351          22        1       0  \n",
       "43771  0.064362  4.171572          22        1       0  \n",
       "43772  0.054260  4.045283          22        1       0  \n",
       "43773  0.055557  3.843736          22        1       0  \n",
       "43774  0.061371  3.793209          22        1       0  \n",
       "\n",
       "[43775 rows x 17 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dat_source.iloc[:,1:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nat_demand</th>\n",
       "      <th>T2M_toc</th>\n",
       "      <th>QV2M_toc</th>\n",
       "      <th>TQL_toc</th>\n",
       "      <th>W2M_toc</th>\n",
       "      <th>T2M_san</th>\n",
       "      <th>QV2M_san</th>\n",
       "      <th>TQL_san</th>\n",
       "      <th>W2M_san</th>\n",
       "      <th>T2M_dav</th>\n",
       "      <th>QV2M_dav</th>\n",
       "      <th>TQL_dav</th>\n",
       "      <th>W2M_dav</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>970.3450</td>\n",
       "      <td>25.865259</td>\n",
       "      <td>0.018576</td>\n",
       "      <td>0.016174</td>\n",
       "      <td>21.850546</td>\n",
       "      <td>23.482446</td>\n",
       "      <td>0.017272</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>10.328949</td>\n",
       "      <td>22.662134</td>\n",
       "      <td>0.016562</td>\n",
       "      <td>0.096100</td>\n",
       "      <td>5.364148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>912.1755</td>\n",
       "      <td>25.899255</td>\n",
       "      <td>0.018653</td>\n",
       "      <td>0.016418</td>\n",
       "      <td>22.166944</td>\n",
       "      <td>23.399255</td>\n",
       "      <td>0.017265</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>10.681517</td>\n",
       "      <td>22.578943</td>\n",
       "      <td>0.016509</td>\n",
       "      <td>0.087646</td>\n",
       "      <td>5.572471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>900.2688</td>\n",
       "      <td>25.937280</td>\n",
       "      <td>0.018768</td>\n",
       "      <td>0.015480</td>\n",
       "      <td>22.454911</td>\n",
       "      <td>23.343530</td>\n",
       "      <td>0.017211</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>10.874924</td>\n",
       "      <td>22.531030</td>\n",
       "      <td>0.016479</td>\n",
       "      <td>0.078735</td>\n",
       "      <td>5.871184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>889.9538</td>\n",
       "      <td>25.957544</td>\n",
       "      <td>0.018890</td>\n",
       "      <td>0.016273</td>\n",
       "      <td>22.110481</td>\n",
       "      <td>23.238794</td>\n",
       "      <td>0.017128</td>\n",
       "      <td>0.002599</td>\n",
       "      <td>10.518620</td>\n",
       "      <td>22.512231</td>\n",
       "      <td>0.016487</td>\n",
       "      <td>0.068390</td>\n",
       "      <td>5.883621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>893.6865</td>\n",
       "      <td>25.973840</td>\n",
       "      <td>0.018981</td>\n",
       "      <td>0.017281</td>\n",
       "      <td>21.186089</td>\n",
       "      <td>23.075403</td>\n",
       "      <td>0.017059</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>9.733589</td>\n",
       "      <td>22.481653</td>\n",
       "      <td>0.016456</td>\n",
       "      <td>0.064362</td>\n",
       "      <td>5.611724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43770</th>\n",
       "      <td>1301.6065</td>\n",
       "      <td>26.635645</td>\n",
       "      <td>0.018421</td>\n",
       "      <td>0.013165</td>\n",
       "      <td>13.184052</td>\n",
       "      <td>25.135645</td>\n",
       "      <td>0.018048</td>\n",
       "      <td>0.064240</td>\n",
       "      <td>3.086798</td>\n",
       "      <td>23.620020</td>\n",
       "      <td>0.016697</td>\n",
       "      <td>0.073425</td>\n",
       "      <td>3.865351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43771</th>\n",
       "      <td>1250.9634</td>\n",
       "      <td>26.495935</td>\n",
       "      <td>0.018162</td>\n",
       "      <td>0.014713</td>\n",
       "      <td>13.443892</td>\n",
       "      <td>24.769373</td>\n",
       "      <td>0.017781</td>\n",
       "      <td>0.058838</td>\n",
       "      <td>3.659980</td>\n",
       "      <td>23.284998</td>\n",
       "      <td>0.016606</td>\n",
       "      <td>0.064362</td>\n",
       "      <td>4.171572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43772</th>\n",
       "      <td>1193.6802</td>\n",
       "      <td>26.354456</td>\n",
       "      <td>0.017980</td>\n",
       "      <td>0.013836</td>\n",
       "      <td>13.442195</td>\n",
       "      <td>24.479456</td>\n",
       "      <td>0.017606</td>\n",
       "      <td>0.038086</td>\n",
       "      <td>3.769294</td>\n",
       "      <td>23.041956</td>\n",
       "      <td>0.016492</td>\n",
       "      <td>0.054260</td>\n",
       "      <td>4.045283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43773</th>\n",
       "      <td>1130.4575</td>\n",
       "      <td>26.166895</td>\n",
       "      <td>0.017965</td>\n",
       "      <td>0.018486</td>\n",
       "      <td>13.420656</td>\n",
       "      <td>24.112207</td>\n",
       "      <td>0.017393</td>\n",
       "      <td>0.020386</td>\n",
       "      <td>3.872397</td>\n",
       "      <td>22.862207</td>\n",
       "      <td>0.016401</td>\n",
       "      <td>0.055557</td>\n",
       "      <td>3.843736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43774</th>\n",
       "      <td>1084.4737</td>\n",
       "      <td>25.976373</td>\n",
       "      <td>0.018072</td>\n",
       "      <td>0.023315</td>\n",
       "      <td>13.749788</td>\n",
       "      <td>23.663873</td>\n",
       "      <td>0.017156</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>4.165276</td>\n",
       "      <td>22.726373</td>\n",
       "      <td>0.016302</td>\n",
       "      <td>0.061371</td>\n",
       "      <td>3.793209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43775 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       nat_demand    T2M_toc  QV2M_toc   TQL_toc    W2M_toc    T2M_san  \\\n",
       "0        970.3450  25.865259  0.018576  0.016174  21.850546  23.482446   \n",
       "1        912.1755  25.899255  0.018653  0.016418  22.166944  23.399255   \n",
       "2        900.2688  25.937280  0.018768  0.015480  22.454911  23.343530   \n",
       "3        889.9538  25.957544  0.018890  0.016273  22.110481  23.238794   \n",
       "4        893.6865  25.973840  0.018981  0.017281  21.186089  23.075403   \n",
       "...           ...        ...       ...       ...        ...        ...   \n",
       "43770   1301.6065  26.635645  0.018421  0.013165  13.184052  25.135645   \n",
       "43771   1250.9634  26.495935  0.018162  0.014713  13.443892  24.769373   \n",
       "43772   1193.6802  26.354456  0.017980  0.013836  13.442195  24.479456   \n",
       "43773   1130.4575  26.166895  0.017965  0.018486  13.420656  24.112207   \n",
       "43774   1084.4737  25.976373  0.018072  0.023315  13.749788  23.663873   \n",
       "\n",
       "       QV2M_san   TQL_san    W2M_san    T2M_dav  QV2M_dav   TQL_dav   W2M_dav  \n",
       "0      0.017272  0.001855  10.328949  22.662134  0.016562  0.096100  5.364148  \n",
       "1      0.017265  0.001327  10.681517  22.578943  0.016509  0.087646  5.572471  \n",
       "2      0.017211  0.001428  10.874924  22.531030  0.016479  0.078735  5.871184  \n",
       "3      0.017128  0.002599  10.518620  22.512231  0.016487  0.068390  5.883621  \n",
       "4      0.017059  0.001729   9.733589  22.481653  0.016456  0.064362  5.611724  \n",
       "...         ...       ...        ...        ...       ...       ...       ...  \n",
       "43770  0.018048  0.064240   3.086798  23.620020  0.016697  0.073425  3.865351  \n",
       "43771  0.017781  0.058838   3.659980  23.284998  0.016606  0.064362  4.171572  \n",
       "43772  0.017606  0.038086   3.769294  23.041956  0.016492  0.054260  4.045283  \n",
       "43773  0.017393  0.020386   3.872397  22.862207  0.016401  0.055557  3.843736  \n",
       "43774  0.017156  0.019531   4.165276  22.726373  0.016302  0.061371  3.793209  \n",
       "\n",
       "[43775 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "norm_df = pd.DataFrame(min_max_scaler.fit_transform(df), \n",
    "                             columns=df.columns, \n",
    "                             index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nat_demand</th>\n",
       "      <th>T2M_toc</th>\n",
       "      <th>QV2M_toc</th>\n",
       "      <th>TQL_toc</th>\n",
       "      <th>W2M_toc</th>\n",
       "      <th>T2M_san</th>\n",
       "      <th>QV2M_san</th>\n",
       "      <th>TQL_san</th>\n",
       "      <th>W2M_san</th>\n",
       "      <th>T2M_dav</th>\n",
       "      <th>QV2M_dav</th>\n",
       "      <th>TQL_dav</th>\n",
       "      <th>W2M_dav</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.104627</td>\n",
       "      <td>-0.917472</td>\n",
       "      <td>0.128975</td>\n",
       "      <td>-1.005302</td>\n",
       "      <td>1.235019</td>\n",
       "      <td>-1.144115</td>\n",
       "      <td>-0.343719</td>\n",
       "      <td>-1.248909</td>\n",
       "      <td>0.837155</td>\n",
       "      <td>-0.849516</td>\n",
       "      <td>-0.223119</td>\n",
       "      <td>-0.593493</td>\n",
       "      <td>1.053902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.405228</td>\n",
       "      <td>-0.896895</td>\n",
       "      <td>0.177423</td>\n",
       "      <td>-1.001593</td>\n",
       "      <td>1.279262</td>\n",
       "      <td>-1.172214</td>\n",
       "      <td>-0.347505</td>\n",
       "      <td>-1.255035</td>\n",
       "      <td>0.924098</td>\n",
       "      <td>-0.884558</td>\n",
       "      <td>-0.256830</td>\n",
       "      <td>-0.689274</td>\n",
       "      <td>1.175793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.466758</td>\n",
       "      <td>-0.873880</td>\n",
       "      <td>0.249594</td>\n",
       "      <td>-1.015848</td>\n",
       "      <td>1.319529</td>\n",
       "      <td>-1.191036</td>\n",
       "      <td>-0.376265</td>\n",
       "      <td>-1.253863</td>\n",
       "      <td>0.971791</td>\n",
       "      <td>-0.904739</td>\n",
       "      <td>-0.276237</td>\n",
       "      <td>-0.790241</td>\n",
       "      <td>1.350571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.520063</td>\n",
       "      <td>-0.861615</td>\n",
       "      <td>0.326570</td>\n",
       "      <td>-1.003795</td>\n",
       "      <td>1.271366</td>\n",
       "      <td>-1.226412</td>\n",
       "      <td>-0.421498</td>\n",
       "      <td>-1.240290</td>\n",
       "      <td>0.883928</td>\n",
       "      <td>-0.912658</td>\n",
       "      <td>-0.271284</td>\n",
       "      <td>-0.907460</td>\n",
       "      <td>1.357848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.500774</td>\n",
       "      <td>-0.851751</td>\n",
       "      <td>0.383954</td>\n",
       "      <td>-0.988498</td>\n",
       "      <td>1.142105</td>\n",
       "      <td>-1.281600</td>\n",
       "      <td>-0.458811</td>\n",
       "      <td>-1.250375</td>\n",
       "      <td>0.690341</td>\n",
       "      <td>-0.925538</td>\n",
       "      <td>-0.291064</td>\n",
       "      <td>-0.953103</td>\n",
       "      <td>1.198760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43770</th>\n",
       "      <td>0.607224</td>\n",
       "      <td>-0.451182</td>\n",
       "      <td>0.031316</td>\n",
       "      <td>-1.051020</td>\n",
       "      <td>0.023153</td>\n",
       "      <td>-0.585721</td>\n",
       "      <td>0.075035</td>\n",
       "      <td>-0.525595</td>\n",
       "      <td>-0.948741</td>\n",
       "      <td>-0.446036</td>\n",
       "      <td>-0.136983</td>\n",
       "      <td>-0.850407</td>\n",
       "      <td>0.176949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43771</th>\n",
       "      <td>0.345517</td>\n",
       "      <td>-0.535744</td>\n",
       "      <td>-0.131908</td>\n",
       "      <td>-1.027494</td>\n",
       "      <td>0.059487</td>\n",
       "      <td>-0.709435</td>\n",
       "      <td>-0.068952</td>\n",
       "      <td>-0.588224</td>\n",
       "      <td>-0.807395</td>\n",
       "      <td>-0.587154</td>\n",
       "      <td>-0.195263</td>\n",
       "      <td>-0.953103</td>\n",
       "      <td>0.356120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43772</th>\n",
       "      <td>0.049496</td>\n",
       "      <td>-0.621377</td>\n",
       "      <td>-0.246890</td>\n",
       "      <td>-1.040821</td>\n",
       "      <td>0.059250</td>\n",
       "      <td>-0.807359</td>\n",
       "      <td>-0.163363</td>\n",
       "      <td>-0.828833</td>\n",
       "      <td>-0.780439</td>\n",
       "      <td>-0.689528</td>\n",
       "      <td>-0.267975</td>\n",
       "      <td>-1.067556</td>\n",
       "      <td>0.282228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43773</th>\n",
       "      <td>-0.277218</td>\n",
       "      <td>-0.734902</td>\n",
       "      <td>-0.256234</td>\n",
       "      <td>-0.970188</td>\n",
       "      <td>0.056238</td>\n",
       "      <td>-0.931403</td>\n",
       "      <td>-0.278448</td>\n",
       "      <td>-1.034058</td>\n",
       "      <td>-0.755014</td>\n",
       "      <td>-0.765241</td>\n",
       "      <td>-0.326168</td>\n",
       "      <td>-1.052860</td>\n",
       "      <td>0.164302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43774</th>\n",
       "      <td>-0.514848</td>\n",
       "      <td>-0.850218</td>\n",
       "      <td>-0.188877</td>\n",
       "      <td>-0.896831</td>\n",
       "      <td>0.102262</td>\n",
       "      <td>-1.082835</td>\n",
       "      <td>-0.406055</td>\n",
       "      <td>-1.043966</td>\n",
       "      <td>-0.682791</td>\n",
       "      <td>-0.822457</td>\n",
       "      <td>-0.389429</td>\n",
       "      <td>-0.986989</td>\n",
       "      <td>0.134738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43775 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       nat_demand   T2M_toc  QV2M_toc   TQL_toc   W2M_toc   T2M_san  QV2M_san  \\\n",
       "0       -1.104627 -0.917472  0.128975 -1.005302  1.235019 -1.144115 -0.343719   \n",
       "1       -1.405228 -0.896895  0.177423 -1.001593  1.279262 -1.172214 -0.347505   \n",
       "2       -1.466758 -0.873880  0.249594 -1.015848  1.319529 -1.191036 -0.376265   \n",
       "3       -1.520063 -0.861615  0.326570 -1.003795  1.271366 -1.226412 -0.421498   \n",
       "4       -1.500774 -0.851751  0.383954 -0.988498  1.142105 -1.281600 -0.458811   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "43770    0.607224 -0.451182  0.031316 -1.051020  0.023153 -0.585721  0.075035   \n",
       "43771    0.345517 -0.535744 -0.131908 -1.027494  0.059487 -0.709435 -0.068952   \n",
       "43772    0.049496 -0.621377 -0.246890 -1.040821  0.059250 -0.807359 -0.163363   \n",
       "43773   -0.277218 -0.734902 -0.256234 -0.970188  0.056238 -0.931403 -0.278448   \n",
       "43774   -0.514848 -0.850218 -0.188877 -0.896831  0.102262 -1.082835 -0.406055   \n",
       "\n",
       "        TQL_san   W2M_san   T2M_dav  QV2M_dav   TQL_dav   W2M_dav  \n",
       "0     -1.248909  0.837155 -0.849516 -0.223119 -0.593493  1.053902  \n",
       "1     -1.255035  0.924098 -0.884558 -0.256830 -0.689274  1.175793  \n",
       "2     -1.253863  0.971791 -0.904739 -0.276237 -0.790241  1.350571  \n",
       "3     -1.240290  0.883928 -0.912658 -0.271284 -0.907460  1.357848  \n",
       "4     -1.250375  0.690341 -0.925538 -0.291064 -0.953103  1.198760  \n",
       "...         ...       ...       ...       ...       ...       ...  \n",
       "43770 -0.525595 -0.948741 -0.446036 -0.136983 -0.850407  0.176949  \n",
       "43771 -0.588224 -0.807395 -0.587154 -0.195263 -0.953103  0.356120  \n",
       "43772 -0.828833 -0.780439 -0.689528 -0.267975 -1.067556  0.282228  \n",
       "43773 -1.034058 -0.755014 -0.765241 -0.326168 -1.052860  0.164302  \n",
       "43774 -1.043966 -0.682791 -0.822457 -0.389429 -0.986989  0.134738  \n",
       "\n",
       "[43775 rows x 13 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from keijzer import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Add, concatenate, Dropout, Activation, Multiply\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D  \n",
    "from tensorflow.keras.layers import Conv1D, AveragePooling1D, MaxPooling1D\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import activations\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "import math\n",
    "\n",
    "class LossHistory(tensorflow.keras.callbacks.Callback):  # history = LossHistory()\n",
    "    def init(self):  # history.init()\n",
    "        self.losses = []\n",
    "        # self.accs = []\n",
    "        self.val_losses = []\n",
    "        # self.val_accs = []\n",
    "        self.rmses = []\n",
    "        self.mses = []\n",
    "        self.maes = []\n",
    "        self.mapes = []\n",
    "        self.val_rmses = []\n",
    "        self.val_mses = []\n",
    "        self.val_maes = []\n",
    "        self.val_mapes = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        # self.accs.append(logs.get('acc'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        # self.val_accs.append(logs.get('val_accuracy\n",
    "        self.rmses.append(logs.get('root_mean_squared_error'))\n",
    "        self.mses.append(logs.get('mean_squared_error'))\n",
    "        self.maes.append(logs.get('mean_absolute_error'))\n",
    "        self.mapes.append(logs.get('mean_absolute_percentage_error'))\n",
    "        self.val_rmses.append(logs.get('val_root_mean_squared_error'))\n",
    "        self.val_mses.append(logs.get('val_mean_squared_error'))\n",
    "        self.val_maes.append(logs.get('val_mean_absolute_error'))\n",
    "        self.val_mapes.append(logs.get('val_mean_absolute_percentage_error'))\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df = np.array(norm_df)\n",
    "\n",
    "def create_dataset(dataset, timesteps, output_timesteps):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - timesteps -output_timesteps - 1):\n",
    "        a = dataset[i:(i + timesteps), :]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[(i + timesteps):(i+timesteps+output_timesteps), :])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "timesteps = 120\n",
    "output_timesteps = 24\n",
    "num_features = 13\n",
    "X, Y = create_dataset(norm_df, timesteps, output_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((43630, 120, 13), (43630, 24, 13))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trX = X[:31210, :, :]\n",
    "vaX = X[31210:, :, :]\n",
    "trY = Y[:31210, :, 0]\n",
    "vaY = Y[31210:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((31210, 120, 13), (12420, 120, 13), (31210, 24), (12420, 24))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trX.shape, vaX.shape, trY.shape, vaY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\smartgrid_AI\\Anaconda3\\envs\\navy\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "res10 : (?, 120, 403)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 120, 13)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "permute (Permute)               (None, 13, 120)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 13, 120)      14520       permute[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 13, 120)      14520       permute[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 13, 120)      0           dense[0][0]                      \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Permute)         (None, 120, 13)      0           multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 120, 13)      0           input_1[0][0]                    \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 120, 256)     6912        multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 120, 256)     0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 120, 256)     0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 120, 256)     0           activation[0][0]                 \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 120, 13)      16653       multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 120, 13)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 120, 13)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 120, 13)      0           activation_2[0][0]               \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 120, 13)      0           input_1[0][0]                    \n",
      "                                                                 multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, 120, 13)      0           input_1[0][0]                    \n",
      "                                                                 multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 120, 256)     6912        add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 120, 256)     6912        subtract[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 120, 256)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 120, 256)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 120, 256)     0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 120, 256)     0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 120, 256)     0           activation_4[0][0]               \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_6 (Multiply)           (None, 120, 256)     0           activation_8[0][0]               \n",
      "                                                                 activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 120, 13)      16653       multiply_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 120, 13)      16653       multiply_6[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 120, 13)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 120, 13)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 120, 13)      0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 120, 13)      0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_5 (Multiply)           (None, 120, 13)      0           activation_6[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multiply_7 (Multiply)           (None, 120, 13)      0           activation_10[0][0]              \n",
      "                                                                 activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 120, 13)      0           add[0][0]                        \n",
      "                                                                 multiply_5[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 120, 13)      0           subtract[0][0]                   \n",
      "                                                                 multiply_7[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 120, 52)      0           add_1[0][0]                      \n",
      "                                                                 subtract_1[0][0]                 \n",
      "                                                                 add[0][0]                        \n",
      "                                                                 subtract[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 120, 256)     26880       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 120, 256)     26880       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 120, 256)     0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 120, 256)     0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 120, 256)     0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 120, 256)     0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_8 (Multiply)           (None, 120, 256)     0           activation_12[0][0]              \n",
      "                                                                 activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_10 (Multiply)          (None, 120, 256)     0           activation_16[0][0]              \n",
      "                                                                 activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 120, 13)      16653       multiply_8[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 120, 13)      16653       multiply_10[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 120, 13)      0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 120, 13)      0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 120, 13)      0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 120, 13)      0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_9 (Multiply)           (None, 120, 13)      0           activation_14[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_11 (Multiply)          (None, 120, 13)      0           activation_18[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 120, 13)      0           add_1[0][0]                      \n",
      "                                                                 multiply_9[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "subtract_2 (Subtract)           (None, 120, 13)      0           subtract_1[0][0]                 \n",
      "                                                                 multiply_11[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 120, 78)      0           add_2[0][0]                      \n",
      "                                                                 subtract_2[0][0]                 \n",
      "                                                                 concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 120, 256)     40192       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 120, 256)     40192       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 120, 256)     0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 120, 256)     0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 120, 256)     0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 120, 256)     0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_12 (Multiply)          (None, 120, 256)     0           activation_20[0][0]              \n",
      "                                                                 activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_14 (Multiply)          (None, 120, 256)     0           activation_24[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 120, 13)      16653       multiply_12[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 120, 13)      16653       multiply_14[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 120, 13)      0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 120, 13)      0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 120, 13)      0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 120, 13)      0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_13 (Multiply)          (None, 120, 13)      0           activation_22[0][0]              \n",
      "                                                                 activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_15 (Multiply)          (None, 120, 13)      0           activation_26[0][0]              \n",
      "                                                                 activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 120, 13)      0           add_2[0][0]                      \n",
      "                                                                 multiply_13[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_3 (Subtract)           (None, 120, 13)      0           subtract_1[0][0]                 \n",
      "                                                                 multiply_15[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 120, 104)     0           add_3[0][0]                      \n",
      "                                                                 subtract_3[0][0]                 \n",
      "                                                                 concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 120, 256)     53504       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 120, 256)     53504       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 120, 256)     0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 120, 256)     0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 120, 256)     0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 120, 256)     0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_16 (Multiply)          (None, 120, 256)     0           activation_28[0][0]              \n",
      "                                                                 activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_18 (Multiply)          (None, 120, 256)     0           activation_32[0][0]              \n",
      "                                                                 activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 120, 13)      16653       multiply_16[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 120, 13)      16653       multiply_18[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 120, 13)      0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 120, 13)      0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 120, 13)      0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 120, 13)      0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_17 (Multiply)          (None, 120, 13)      0           activation_30[0][0]              \n",
      "                                                                 activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_19 (Multiply)          (None, 120, 13)      0           activation_34[0][0]              \n",
      "                                                                 activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 120, 13)      0           add_3[0][0]                      \n",
      "                                                                 multiply_17[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_4 (Subtract)           (None, 120, 13)      0           subtract_3[0][0]                 \n",
      "                                                                 multiply_19[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 120, 130)     0           add_4[0][0]                      \n",
      "                                                                 subtract_4[0][0]                 \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 120, 256)     66816       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 120, 256)     66816       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 120, 256)     0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 120, 256)     0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 120, 256)     0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 120, 256)     0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_20 (Multiply)          (None, 120, 256)     0           activation_36[0][0]              \n",
      "                                                                 activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_22 (Multiply)          (None, 120, 256)     0           activation_40[0][0]              \n",
      "                                                                 activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 120, 13)      16653       multiply_20[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 120, 13)      16653       multiply_22[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 120, 13)      0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 120, 13)      0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 120, 13)      0           conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 120, 13)      0           conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_21 (Multiply)          (None, 120, 13)      0           activation_38[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "multiply_23 (Multiply)          (None, 120, 13)      0           activation_42[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 120, 13)      0           add_4[0][0]                      \n",
      "                                                                 multiply_21[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "subtract_5 (Subtract)           (None, 120, 13)      0           subtract_4[0][0]                 \n",
      "                                                                 multiply_23[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 120, 26)      0           add_5[0][0]                      \n",
      "                                                                 subtract_5[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 120, 403)     0           concatenate[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 concatenate_3[0][0]              \n",
      "                                                                 concatenate_4[0][0]              \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 120, 720)     290880      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 120, 720)     0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 120, 240)     173040      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 120, 240)     0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 240)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 24)           5784        global_average_pooling1d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 1,077,447\n",
      "Trainable params: 1,077,447\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    alpha=1.0\n",
    "    gamma=1.2\n",
    "    beta = 1.1785\n",
    "    num_features = 13\n",
    "    hfilters = 256\n",
    "    hkernel_size1 = 2\n",
    "    hkernel_size2 = 4\n",
    "    visible1 = Input(shape=(timesteps, num_features))\n",
    "    \n",
    "    per1 = Permute((2,1))(visible1)\n",
    "    den1a = Dense(timesteps, activation='tanh')(per1)\n",
    "    den1b = Dense(timesteps, activation='sigmoid')(per1)\n",
    "    den1 = Multiply()([den1a, den1b])\n",
    "    per2 = Permute((2,1), name='attention_vec')(den1)\n",
    "    mul1 = Multiply()([visible1, per2])\n",
    "    \n",
    "    \n",
    "    d1 = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=1)(mul1)\n",
    "    d1a = Activation(activations.tanh)(d1)\n",
    "    d1b = Activation(activations.sigmoid)(d1)\n",
    "    d1 = Multiply()([d1a, d1b])\n",
    "    \n",
    "    d1 = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=2)(d1)\n",
    "    d1a = Activation(activations.tanh)(d1)\n",
    "    d1b = Activation(activations.sigmoid)(d1)\n",
    "    d1 = Multiply()([d1a, d1b])\n",
    "    \n",
    "    res01a = Add()([visible1, d1])   # (100, 25) (100, 25)\n",
    "    res01b = Subtract()([visible1, d1])\n",
    "\n",
    "    \n",
    "    d1 = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=2)(res01a)\n",
    "    d1a = Activation(activations.tanh)(d1)\n",
    "    d1b = Activation(activations.sigmoid)(d1)\n",
    "    d1 = Multiply()([d1a, d1b])\n",
    "    \n",
    "    d1 = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=4)(d1)    \n",
    "    d1a = Activation(activations.tanh)(d1)\n",
    "    d1b = Activation(activations.sigmoid)(d1)\n",
    "    \n",
    "    d1 = Multiply()([d1a, d1b])\n",
    "    res02a = Add()([res01a, d1])   # (100, 25) (100, 25)\n",
    "    \n",
    "    d2 = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=2)(res01b) \n",
    "    d2a = Activation(activations.tanh)(d2)\n",
    "    d2b = Activation(activations.sigmoid)(d2)\n",
    "    d2 = Multiply()([d2a, d2b])\n",
    "    \n",
    "    d2 = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=4)(d2) \n",
    "    d2a = Activation(activations.tanh)(d2)\n",
    "    d2b = Activation(activations.sigmoid)(d2)\n",
    "    d2 = Multiply()([d2a, d2b])\n",
    "    \n",
    "    res02b = Subtract()([res01b, d2])   # (100, 25) (100, 25) \n",
    "    res02 = Concatenate()([res02a, res02b, res01a, res01b])\n",
    "    \n",
    "    d1 = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=4)(res02)\n",
    "    d1a = Activation(activations.tanh)(d1)\n",
    "    d1b = Activation(activations.sigmoid)(d1)\n",
    "    d1 = Multiply()([d1a, d1b])\n",
    "    \n",
    "    d1 = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=8)(d1)    \n",
    "    d1a = Activation(activations.tanh)(d1)\n",
    "    d1b = Activation(activations.sigmoid)(d1)\n",
    "    d1 = Multiply()([d1a, d1b])\n",
    "    \n",
    "    res03a = Add()([res02a, d1])   # (100, 25) (100, 25)\n",
    "    \n",
    "    d2 = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=4)(res02) \n",
    "    d2a = Activation(activations.tanh)(d2)\n",
    "    d2b = Activation(activations.sigmoid)(d2)\n",
    "    d2 = Multiply()([d2a, d2b])\n",
    "    \n",
    "    d2 = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=8)(d2) \n",
    "    d2a = Activation(activations.tanh)(d2)\n",
    "    d2b = Activation(activations.sigmoid)(d2)\n",
    "    d2 = Multiply()([d2a, d2b])\n",
    "    \n",
    "    res03b = Subtract()([res02b, d2])   # (100, 25) (100, 25)\n",
    "    res03 = Concatenate()([res03a, res03b, res02])\n",
    "    \n",
    "    d1 = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=8)(res03)\n",
    "    d1a = Activation(activations.tanh)(d1)\n",
    "    d1b = Activation(activations.sigmoid)(d1)\n",
    "    d1 = Multiply()([d1a, d1b])\n",
    "    d1 = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=4)(d1)    \n",
    "    d1a = Activation(activations.tanh)(d1)\n",
    "    d1b = Activation(activations.sigmoid)(d1)\n",
    "    d1 = Multiply()([d1a, d1b])\n",
    "    res04a = Add()([res03a, d1])   # (100, 25) (100, 25)\n",
    "    \n",
    "    d2 = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=8)(res03) \n",
    "    d2a = Activation(activations.tanh)(d2)\n",
    "    d2b = Activation(activations.sigmoid)(d2)\n",
    "    d2 = Multiply()([d2a, d2b])\n",
    "    \n",
    "    d2 = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=4)(d2) \n",
    "    d2a = Activation(activations.tanh)(d2)\n",
    "    d2b = Activation(activations.sigmoid)(d2)\n",
    "    d2 = Multiply()([d2a, d2b])\n",
    "    \n",
    "    res04b = Subtract()([res02b, d2])   # (100, 25) (100, 25)\n",
    "    res04 = Concatenate()([res04a, res04b, res03])\n",
    "    \n",
    "    d1 = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=4)(res04)\n",
    "    d1a = Activation(activations.tanh)(d1)\n",
    "    d1b = Activation(activations.sigmoid)(d1)\n",
    "    d1 = Multiply()([d1a, d1b])\n",
    "    \n",
    "    d1 = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=2)(d1)    \n",
    "    d1a = Activation(activations.tanh)(d1)\n",
    "    d1b = Activation(activations.sigmoid)(d1)\n",
    "    d1 = Multiply()([d1a, d1b])\n",
    "    \n",
    "    res05a = Add()([res04a, d1])   # (100, 25) (100, 25)\n",
    "    \n",
    "    d2 = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=4)(res04) \n",
    "    d2a = Activation(activations.tanh)(d2)\n",
    "    d2b = Activation(activations.sigmoid)(d2)\n",
    "    d2 = Multiply()([d2a, d2b])\n",
    "    \n",
    "    d2 = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=2)(d2) \n",
    "    d2a = Activation(activations.tanh)(d2)\n",
    "    d2b = Activation(activations.sigmoid)(d2)\n",
    "    d2 = Multiply()([d2a, d2b])\n",
    "    \n",
    "    res05b = Subtract()([res04b, d2])   # (100, 25) (100, 25)\n",
    "    res05 = Concatenate()([res05a, res05b, res04])\n",
    "    \n",
    "    d1 = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=2)(res05)\n",
    "    d1a = Activation(activations.tanh)(d1)\n",
    "    d1b = Activation(activations.sigmoid)(d1)\n",
    "    d1 = Multiply()([d1a, d1b])\n",
    "    \n",
    "    d1 = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=1)(d1)    \n",
    "    d1a = Activation(activations.tanh)(d1)\n",
    "    d1b = Activation(activations.sigmoid)(d1)\n",
    "    d1 = Multiply()([d1a, d1b])\n",
    "\n",
    "    res06a = Add()([res05a, d1])   # (100, 25) (100, 25)\n",
    "    \n",
    "    d2 = Conv1D(filters=round(hfilters*alpha), kernel_size=round(hkernel_size1*beta), padding='causal', dilation_rate=2)(res05) \n",
    "    d2a = Activation(activations.tanh)(d2)\n",
    "    d2b = Activation(activations.sigmoid)(d2)\n",
    "    d2 = Multiply()([d2a, d2b])\n",
    "    d2 = Conv1D(filters=num_features, kernel_size=round(hkernel_size2*beta), padding='causal', dilation_rate=1)(d2) \n",
    "    d2a = Activation(activations.tanh)(d2)\n",
    "    d2b = Activation(activations.sigmoid)(d2)\n",
    "    d2 = Multiply()([d2a, d2b])\n",
    "\n",
    "    res06b = Subtract()([res05b, d2])   # (100, 25) (100, 25)\n",
    "    res06 = Concatenate()([res06a, res06b])\n",
    "    \n",
    "    res10 = Concatenate()([res02, res03, res04, res05, res06, visible1])   # \n",
    "    \n",
    "    print('res10 :', res10.shape)  # (None, 24, 11) \n",
    "    \n",
    "    out = Conv1D(720, 1, padding='same', activation='relu')(res10)   # 256, 11X10=110\n",
    "    out = Dropout(0.2)(out)   #SpatialDropout1D\n",
    "    \n",
    "    out = Conv1D(240, 1, padding='same', activation='relu')(out) # 512,  110X5=550\n",
    "    out = Dropout(0.2)(out)\n",
    "    \n",
    "    out = GlobalAveragePooling1D()(out) # pool_size=2, strides=1\n",
    "    \n",
    "    out = Dense(24)(out) \n",
    "    model = Model(inputs=[visible1], outputs=[out])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    model.compile(loss='mae', optimizer='adam', metrics=['mse', 'mae'])\n",
    "    early_stopping =EarlyStopping(monitor='val_loss', patience=150)\n",
    "    batch_size = 100\n",
    "    epochs = 1000\n",
    "\n",
    "    history = LossHistory()\n",
    "    history.init()\n",
    "    \n",
    "    #hist = model.fit(trX, trY, epochs=epochs, batch_size=batch_size, shuffle=False, validation_data=(vaX, vaY), callbacks=[history, early_stopping])  # , checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 31210 samples, validate on 12420 samples\n",
      "Epoch 1/1000\n",
      "31210/31210 [==============================] - 20s 632us/sample - loss: 0.5431 - mean_squared_error: 0.5167 - mean_absolute_error: 0.5431 - val_loss: 0.4446 - val_mean_squared_error: 0.3600 - val_mean_absolute_error: 0.4446 - loss: 0.8943 - mean_squared_error: 1.1562 - mean_absolut - ETA: 22s - loss: 0.8076 - mean_squa\n",
      "Epoch 2/1000\n",
      "31210/31210 [==============================] - 16s 502us/sample - loss: 0.4304 - mean_squared_error: 0.3510 - mean_absolute_error: 0.4304 - val_loss: 0.4203 - val_mean_squared_error: 0.3382 - val_mean_absolute_error: 0.4203\n",
      "Epoch 3/1000\n",
      "31210/31210 [==============================] - 16s 503us/sample - loss: 0.4086 - mean_squared_error: 0.3257 - mean_absolute_error: 0.4086 - val_loss: 0.4234 - val_mean_squared_error: 0.3449 - val_mean_absolute_error: 0.4234\n",
      "Epoch 4/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.3897 - mean_squared_error: 0.2988 - mean_absolute_error: 0.3897 - val_loss: 0.4203 - val_mean_squared_error: 0.3240 - val_mean_absolute_error: 0.4203\n",
      "Epoch 5/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.3592 - mean_squared_error: 0.2574 - mean_absolute_error: 0.3592 - val_loss: 0.3774 - val_mean_squared_error: 0.2649 - val_mean_absolute_error: 0.3774\n",
      "Epoch 6/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.3264 - mean_squared_error: 0.2149 - mean_absolute_error: 0.3264 - val_loss: 0.3473 - val_mean_squared_error: 0.2319 - val_mean_absolute_error: 0.3473\n",
      "Epoch 7/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.3021 - mean_squared_error: 0.1851 - mean_absolute_error: 0.3021 - val_loss: 0.3408 - val_mean_squared_error: 0.2244 - val_mean_absolute_error: 0.3408\n",
      "Epoch 8/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.2823 - mean_squared_error: 0.1631 - mean_absolute_error: 0.2823 - val_loss: 0.3248 - val_mean_squared_error: 0.2081 - val_mean_absolute_error: 0.3248\n",
      "Epoch 9/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.2691 - mean_squared_error: 0.1490 - mean_absolute_error: 0.2691 - val_loss: 0.3198 - val_mean_squared_error: 0.2002 - val_mean_absolute_error: 0.3198\n",
      "Epoch 10/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.2550 - mean_squared_error: 0.1358 - mean_absolute_error: 0.2550 - val_loss: 0.3185 - val_mean_squared_error: 0.1972 - val_mean_absolute_error: 0.3185\n",
      "Epoch 11/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.2462 - mean_squared_error: 0.1265 - mean_absolute_error: 0.2462 - val_loss: 0.3377 - val_mean_squared_error: 0.2236 - val_mean_absolute_error: 0.3377\n",
      "Epoch 12/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.2380 - mean_squared_error: 0.1185 - mean_absolute_error: 0.2380 - val_loss: 0.3379 - val_mean_squared_error: 0.2345 - val_mean_absolute_error: 0.3379\n",
      "Epoch 13/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.2284 - mean_squared_error: 0.1097 - mean_absolute_error: 0.2284 - val_loss: 0.3218 - val_mean_squared_error: 0.2121 - val_mean_absolute_error: 0.3218\n",
      "Epoch 14/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.2241 - mean_squared_error: 0.1049 - mean_absolute_error: 0.2241 - val_loss: 0.3071 - val_mean_squared_error: 0.1997 - val_mean_absolute_error: 0.3071\n",
      "Epoch 15/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.2243 - mean_squared_error: 0.1037 - mean_absolute_error: 0.2243 - val_loss: 0.2965 - val_mean_squared_error: 0.1836 - val_mean_absolute_error: 0.2965\n",
      "Epoch 16/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.2215 - mean_squared_error: 0.1014 - mean_absolute_error: 0.2215 - val_loss: 0.3050 - val_mean_squared_error: 0.1837 - val_mean_absolute_error: 0.3050\n",
      "Epoch 17/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.2140 - mean_squared_error: 0.0947 - mean_absolute_error: 0.2140 - val_loss: 0.3096 - val_mean_squared_error: 0.1891 - val_mean_absolute_error: 0.3096\n",
      "Epoch 18/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.2069 - mean_squared_error: 0.0892 - mean_absolute_error: 0.2069 - val_loss: 0.3065 - val_mean_squared_error: 0.1881 - val_mean_absolute_error: 0.3065\n",
      "Epoch 19/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.2031 - mean_squared_error: 0.0851 - mean_absolute_error: 0.2031 - val_loss: 0.3035 - val_mean_squared_error: 0.1884 - val_mean_absolute_error: 0.3035\n",
      "Epoch 20/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1996 - mean_squared_error: 0.0824 - mean_absolute_error: 0.1996 - val_loss: 0.3182 - val_mean_squared_error: 0.2020 - val_mean_absolute_error: 0.3182\n",
      "Epoch 21/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1998 - mean_squared_error: 0.0814 - mean_absolute_error: 0.1998 - val_loss: 0.3112 - val_mean_squared_error: 0.1946 - val_mean_absolute_error: 0.3112\n",
      "Epoch 22/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1990 - mean_squared_error: 0.0796 - mean_absolute_error: 0.1990 - val_loss: 0.2987 - val_mean_squared_error: 0.1875 - val_mean_absolute_error: 0.2987\n",
      "Epoch 23/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1967 - mean_squared_error: 0.0777 - mean_absolute_error: 0.1967 - val_loss: 0.3157 - val_mean_squared_error: 0.2017 - val_mean_absolute_error: 0.3157uared_error: 0.0781 - mean_absolute_error\n",
      "Epoch 24/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1963 - mean_squared_error: 0.0767 - mean_absolute_error: 0.1963 - val_loss: 0.3061 - val_mean_squared_error: 0.1933 - val_mean_absolute_error: 0.3061\n",
      "Epoch 25/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1930 - mean_squared_error: 0.0745 - mean_absolute_error: 0.1930 - val_loss: 0.2946 - val_mean_squared_error: 0.1834 - val_mean_absolute_error: 0.2946\n",
      "Epoch 26/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1928 - mean_squared_error: 0.0750 - mean_absolute_error: 0.1928 - val_loss: 0.2947 - val_mean_squared_error: 0.1800 - val_mean_absolute_error: 0.2947\n",
      "Epoch 27/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1847 - mean_squared_error: 0.0685 - mean_absolute_error: 0.1847 - val_loss: 0.2926 - val_mean_squared_error: 0.1866 - val_mean_absolute_error: 0.2926- mean - ETA: 0s - loss: 0.1850 - mean_squared_error: 0.0689 - mean_absolute_error: \n",
      "Epoch 28/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1929 - mean_squared_error: 0.0737 - mean_absolute_error: 0.1929 - val_loss: 0.3082 - val_mean_squared_error: 0.1935 - val_mean_absolute_error: 0.3082\n",
      "Epoch 29/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1941 - mean_squared_error: 0.0746 - mean_absolute_error: 0.1941 - val_loss: 0.3184 - val_mean_squared_error: 0.1996 - val_mean_absolute_error: 0.3184\n",
      "Epoch 30/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1965 - mean_squared_error: 0.0771 - mean_absolute_error: 0.1965 - val_loss: 0.2958 - val_mean_squared_error: 0.1822 - val_mean_absolute_error: 0.2958\n",
      "Epoch 31/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1824 - mean_squared_error: 0.0672 - mean_absolute_error: 0.1824 - val_loss: 0.2967 - val_mean_squared_error: 0.1828 - val_mean_absolute_error: 0.2967\n",
      "Epoch 32/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1799 - mean_squared_error: 0.0650 - mean_absolute_error: 0.1799 - val_loss: 0.2973 - val_mean_squared_error: 0.1826 - val_mean_absolute_error: 0.2973\n",
      "Epoch 33/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1788 - mean_squared_error: 0.0632 - mean_absolute_error: 0.1788 - val_loss: 0.2955 - val_mean_squared_error: 0.1767 - val_mean_absolute_error: 0.2955\n",
      "Epoch 34/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1793 - mean_squared_error: 0.0635 - mean_absolute_error: 0.1793 - val_loss: 0.2866 - val_mean_squared_error: 0.1655 - val_mean_absolute_error: 0.2866\n",
      "Epoch 35/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1818 - mean_squared_error: 0.0645 - mean_absolute_error: 0.1818 - val_loss: 0.2842 - val_mean_squared_error: 0.1683 - val_mean_absolute_error: 0.2842n_squ\n",
      "Epoch 36/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1725 - mean_squared_error: 0.0581 - mean_absolute_error: 0.1725 - val_loss: 0.3101 - val_mean_squared_error: 0.1869 - val_mean_absolute_error: 0.3101\n",
      "Epoch 37/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1814 - mean_squared_error: 0.0638 - mean_absolute_error: 0.1814 - val_loss: 0.2894 - val_mean_squared_error: 0.1708 - val_mean_absolute_error: 0.2894\n",
      "Epoch 38/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1738 - mean_squared_error: 0.0595 - mean_absolute_error: 0.1738 - val_loss: 0.2947 - val_mean_squared_error: 0.1782 - val_mean_absolute_error: 0.2947\n",
      "Epoch 39/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1726 - mean_squared_error: 0.0586 - mean_absolute_error: 0.1726 - val_loss: 0.2982 - val_mean_squared_error: 0.1793 - val_mean_absolute_error: 0.2982\n",
      "Epoch 40/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1679 - mean_squared_error: 0.0556 - mean_absolute_error: 0.1679 - val_loss: 0.2978 - val_mean_squared_error: 0.1814 - val_mean_absolute_error: 0.2978\n",
      "Epoch 41/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1719 - mean_squared_error: 0.0579 - mean_absolute_error: 0.1719 - val_loss: 0.3037 - val_mean_squared_error: 0.1840 - val_mean_absolute_error: 0.3037\n",
      "Epoch 42/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1726 - mean_squared_error: 0.0576 - mean_absolute_error: 0.1726 - val_loss: 0.2969 - val_mean_squared_error: 0.1818 - val_mean_absolute_error: 0.2969\n",
      "Epoch 43/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1725 - mean_squared_error: 0.0574 - mean_absolute_error: 0.1725 - val_loss: 0.3013 - val_mean_squared_error: 0.1813 - val_mean_absolute_error: 0.3013\n",
      "Epoch 44/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1688 - mean_squared_error: 0.0544 - mean_absolute_error: 0.1688 - val_loss: 0.2998 - val_mean_squared_error: 0.1733 - val_mean_absolute_error: 0.2998\n",
      "Epoch 45/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1745 - mean_squared_error: 0.0584 - mean_absolute_error: 0.1745 - val_loss: 0.3172 - val_mean_squared_error: 0.2007 - val_mean_absolute_error: 0.3172\n",
      "Epoch 46/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1673 - mean_squared_error: 0.0544 - mean_absolute_error: 0.1673 - val_loss: 0.2978 - val_mean_squared_error: 0.1817 - val_mean_absolute_error: 0.2978\n",
      "Epoch 47/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1615 - mean_squared_error: 0.0509 - mean_absolute_error: 0.1615 - val_loss: 0.2850 - val_mean_squared_error: 0.1736 - val_mean_absolute_error: 0.28501631 - \n",
      "Epoch 48/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1593 - mean_squared_error: 0.0492 - mean_absolute_error: 0.1593 - val_loss: 0.2829 - val_mean_squared_error: 0.1670 - val_mean_absolute_error: 0.2829\n",
      "Epoch 49/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1622 - mean_squared_error: 0.0500 - mean_absolute_error: 0.1622 - val_loss: 0.3017 - val_mean_squared_error: 0.1787 - val_mean_absolute_error: 0.3017\n",
      "Epoch 50/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1608 - mean_squared_error: 0.0498 - mean_absolute_error: 0.1608 - val_loss: 0.3073 - val_mean_squared_error: 0.1955 - val_mean_absolute_error: 0.3073\n",
      "Epoch 51/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1538 - mean_squared_error: 0.0462 - mean_absolute_error: 0.1538 - val_loss: 0.2960 - val_mean_squared_error: 0.1825 - val_mean_absolute_error: 0.2960\n",
      "Epoch 52/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1548 - mean_squared_error: 0.0466 - mean_absolute_error: 0.1548 - val_loss: 0.2911 - val_mean_squared_error: 0.1767 - val_mean_absolute_error: 0.2911\n",
      "Epoch 53/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1531 - mean_squared_error: 0.0456 - mean_absolute_error: 0.1531 - val_loss: 0.2836 - val_mean_squared_error: 0.1704 - val_mean_absolute_error: 0.2836\n",
      "Epoch 54/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1574 - mean_squared_error: 0.0481 - mean_absolute_error: 0.1574 - val_loss: 0.3016 - val_mean_squared_error: 0.1824 - val_mean_absolute_error: 0.3016oss: 0.1747 - mean_squared_error: 0.056 - ETA: 10s - lo\n",
      "Epoch 55/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1547 - mean_squared_error: 0.0460 - mean_absolute_error: 0.1547 - val_loss: 0.3042 - val_mean_squared_error: 0.1922 - val_mean_absolute_error: 0.3042\n",
      "Epoch 56/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1508 - mean_squared_error: 0.0442 - mean_absolute_error: 0.1508 - val_loss: 0.3123 - val_mean_squared_error: 0.2021 - val_mean_absolute_error: 0.3123\n",
      "Epoch 57/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1512 - mean_squared_error: 0.0437 - mean_absolute_error: 0.1512 - val_loss: 0.2925 - val_mean_squared_error: 0.1799 - val_mean_absolute_error: 0.2925\n",
      "Epoch 58/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1514 - mean_squared_error: 0.0441 - mean_absolute_error: 0.1514 - val_loss: 0.2802 - val_mean_squared_error: 0.1657 - val_mean_absolute_error: 0.2802\n",
      "Epoch 59/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1526 - mean_squared_error: 0.0443 - mean_absolute_error: 0.1526 - val_loss: 0.3112 - val_mean_squared_error: 0.1922 - val_mean_absolute_error: 0.3112\n",
      "Epoch 60/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1544 - mean_squared_error: 0.0453 - mean_absolute_error: 0.1544 - val_loss: 0.3130 - val_mean_squared_error: 0.2015 - val_mean_absolute_error: 0.3130\n",
      "Epoch 61/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1517 - mean_squared_error: 0.0447 - mean_absolute_error: 0.1517 - val_loss: 0.3122 - val_mean_squared_error: 0.1970 - val_mean_absolute_error: 0.3122\n",
      "Epoch 62/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1569 - mean_squared_error: 0.0462 - mean_absolute_error: 0.1569 - val_loss: 0.2936 - val_mean_squared_error: 0.1775 - val_mean_absolute_error: 0.2936\n",
      "Epoch 63/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1562 - mean_squared_error: 0.0466 - mean_absolute_error: 0.1562 - val_loss: 0.3025 - val_mean_squared_error: 0.1882 - val_mean_absolute_error: 0.3025\n",
      "Epoch 64/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1545 - mean_squared_error: 0.0454 - mean_absolute_error: 0.1545 - val_loss: 0.3188 - val_mean_squared_error: 0.2045 - val_mean_absolute_error: 0.3188\n",
      "Epoch 65/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1516 - mean_squared_error: 0.0439 - mean_absolute_error: 0.1516 - val_loss: 0.3142 - val_mean_squared_error: 0.2042 - val_mean_absolute_error: 0.3142\n",
      "Epoch 66/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1540 - mean_squared_error: 0.0461 - mean_absolute_error: 0.1540 - val_loss: 0.3189 - val_mean_squared_error: 0.2046 - val_mean_absolute_error: 0.3189\n",
      "Epoch 67/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1550 - mean_squared_error: 0.0454 - mean_absolute_error: 0.1550 - val_loss: 0.3128 - val_mean_squared_error: 0.1978 - val_mean_absolute_error: 0.3128\n",
      "Epoch 68/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1563 - mean_squared_error: 0.0460 - mean_absolute_error: 0.1563 - val_loss: 0.2863 - val_mean_squared_error: 0.1708 - val_mean_absolute_error: 0.2863\n",
      "Epoch 69/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1544 - mean_squared_error: 0.0455 - mean_absolute_error: 0.1544 - val_loss: 0.3160 - val_mean_squared_error: 0.2056 - val_mean_absolute_error: 0.3160\n",
      "Epoch 70/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1487 - mean_squared_error: 0.0426 - mean_absolute_error: 0.1487 - val_loss: 0.2956 - val_mean_squared_error: 0.1840 - val_mean_absolute_error: 0.2956n_squared_error: 0. - ETA: 2s - loss: 0.1 - ETA: 0s - loss: 0.1492 - mean_squared_error: 0.0430 - mean_absolute_err\n",
      "Epoch 71/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1426 - mean_squared_error: 0.0391 - mean_absolute_error: 0.1426 - val_loss: 0.2928 - val_mean_squared_error: 0.1814 - val_mean_absolute_error: 0.2928\n",
      "Epoch 72/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1426 - mean_squared_error: 0.0388 - mean_absolute_error: 0.1426 - val_loss: 0.2871 - val_mean_squared_error: 0.1726 - val_mean_absolute_error: 0.2871 0.1427 - mean_squared_error: 0.0389 - mean_absolute_error: 0.14\n",
      "Epoch 73/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1470 - mean_squared_error: 0.0409 - mean_absolute_error: 0.1470 - val_loss: 0.2943 - val_mean_squared_error: 0.1810 - val_mean_absolute_error: 0.2943\n",
      "Epoch 74/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1516 - mean_squared_error: 0.0435 - mean_absolute_error: 0.1516 - val_loss: 0.2950 - val_mean_squared_error: 0.1801 - val_mean_absolute_error: 0.2950 - mean_squared_error: 0.0441 - mean_ab\n",
      "Epoch 75/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1436 - mean_squared_error: 0.0394 - mean_absolute_error: 0.1436 - val_loss: 0.3118 - val_mean_squared_error: 0.1980 - val_mean_absolute_error: 0.3118\n",
      "Epoch 76/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1444 - mean_squared_error: 0.0395 - mean_absolute_error: 0.1444 - val_loss: 0.3232 - val_mean_squared_error: 0.2137 - val_mean_absolute_error: 0.3232\n",
      "Epoch 77/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1461 - mean_squared_error: 0.0412 - mean_absolute_error: 0.1461 - val_loss: 0.2911 - val_mean_squared_error: 0.1767 - val_mean_absolute_error: 0.2911\n",
      "Epoch 78/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1492 - mean_squared_error: 0.0426 - mean_absolute_error: 0.1492 - val_loss: 0.2820 - val_mean_squared_error: 0.1733 - val_mean_absolute_error: 0.2820\n",
      "Epoch 79/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1481 - mean_squared_error: 0.0422 - mean_absolute_error: 0.1481 - val_loss: 0.2874 - val_mean_squared_error: 0.1781 - val_mean_absolute_error: 0.2874\n",
      "Epoch 80/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1426 - mean_squared_error: 0.0387 - mean_absolute_error: 0.1426 - val_loss: 0.3006 - val_mean_squared_error: 0.1893 - val_mean_absolute_error: 0.3006\n",
      "Epoch 81/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1429 - mean_squared_error: 0.0388 - mean_absolute_error: 0.1429 - val_loss: 0.3201 - val_mean_squared_error: 0.2119 - val_mean_absolute_error: 0.3201\n",
      "Epoch 82/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1444 - mean_squared_error: 0.0396 - mean_absolute_error: 0.1444 - val_loss: 0.3140 - val_mean_squared_error: 0.2041 - val_mean_absolute_error: 0.3140\n",
      "Epoch 83/1000\n",
      "31210/31210 [==============================] - 16s 517us/sample - loss: 0.1422 - mean_squared_error: 0.0384 - mean_absolute_error: 0.1422 - val_loss: 0.3074 - val_mean_squared_error: 0.1988 - val_mean_absolute_error: 0.3074_absolute_error:  - ETA: 1s - loss: 0.1428 - mean_squared_error: 0.0388 - mean_ - ETA: 0s - loss: 0.1424 - mean_squared_error: 0.0386 - mean_absolute_error: \n",
      "Epoch 84/1000\n",
      "31210/31210 [==============================] - 16s 520us/sample - loss: 0.1446 - mean_squared_error: 0.0394 - mean_absolute_error: 0.1446 - val_loss: 0.3003 - val_mean_squared_error: 0.1880 - val_mean_absolute_error: 0.3003\n",
      "Epoch 85/1000\n",
      "31210/31210 [==============================] - 16s 512us/sample - loss: 0.1471 - mean_squared_error: 0.0411 - mean_absolute_error: 0.1471 - val_loss: 0.2917 - val_mean_squared_error: 0.1806 - val_mean_absolute_error: 0.29171468 - mean_squared_error: 0.0417 - mea - ETA: 2s - loss: 0.1464 \n",
      "Epoch 86/1000\n",
      "31210/31210 [==============================] - 16s 512us/sample - loss: 0.1422 - mean_squared_error: 0.0386 - mean_absolute_error: 0.1422 - val_loss: 0.3013 - val_mean_squared_error: 0.1943 - val_mean_absolute_error: 0.3013_squared_error: 0.0429 - mean_absolute_error: 0 - ET - ETA: 6s - loss: 0.1445 - mean_squared_error: 0. - ETA: 5s - loss: 0.1446 - mean_squared_e\n",
      "Epoch 87/1000\n",
      "31210/31210 [==============================] - 16s 512us/sample - loss: 0.1427 - mean_squared_error: 0.0385 - mean_absolute_error: 0.1427 - val_loss: 0.3164 - val_mean_squared_error: 0.2117 - val_mean_absolute_error: 0.3164ror: 0.0391 - mean_absolute_error - ETA: 1s - loss: 0.1435 - mean_squared_error: 0.0390 - mean_absolute_error - ETA: 0s - loss: 0.1432 - mean_squared_error: 0.0391 - mean_abso\n",
      "Epoch 88/1000\n",
      "31210/31210 [==============================] - 16s 514us/sample - loss: 0.1479 - mean_squared_error: 0.0418 - mean_absolute_error: 0.1479 - val_loss: 0.3256 - val_mean_squared_error: 0.2085 - val_mean_absolute_error: 0.3256\n",
      "Epoch 89/1000\n",
      "31210/31210 [==============================] - 16s 513us/sample - loss: 0.1454 - mean_squared_error: 0.0398 - mean_absolute_error: 0.1454 - val_loss: 0.3050 - val_mean_squared_error: 0.1813 - val_mean_absolute_error: 0.3050\n",
      "Epoch 90/1000\n",
      "31210/31210 [==============================] - 16s 513us/sample - loss: 0.1399 - mean_squared_error: 0.0377 - mean_absolute_error: 0.1399 - val_loss: 0.3034 - val_mean_squared_error: 0.1828 - val_mean_absolute_error: 0.3034\n",
      "Epoch 91/1000\n",
      "31210/31210 [==============================] - 16s 514us/sample - loss: 0.1364 - mean_squared_error: 0.0356 - mean_absolute_error: 0.1364 - val_loss: 0.3003 - val_mean_squared_error: 0.1912 - val_mean_absolute_error: 0.3003 0.1565 - mean_squared_error: 0.0432 - mean_absolute_error: 0.156 - ETA: 12s - loss: 0.1555 - mean_squared_error - ETA: 9s - loss: 0.1421 - mean_squared_error: 0. - ETA: 8s - loss: 0.1391 - mean_squared_error: 0.0346 - mean_absolute_e - ETA: 7s - loss: 0.1376 - mean_squared_error: 0.0338 - mean_absolute_error: 0.13 - ETA: 7s - loss: 0.1373 - mean_squar - ETA: 5s - loss: 0.1381 - mean_squared_error: 0.0345 - - ETA: 4s - loss: 0.1385 - mean_squared_err - ETA: 2s - loss: 0.1360 - \n",
      "Epoch 92/1000\n",
      "31210/31210 [==============================] - 16s 514us/sample - loss: 0.1366 - mean_squared_error: 0.0358 - mean_absolute_error: 0.1366 - val_loss: 0.3067 - val_mean_squared_error: 0.1989 - val_mean_absolute_error: 0.3067_squared_error: 0.0387 - mean_absolute_error - ETA: 9s - loss: 0.1456 - mean_squared_error: 0.0386 - m - ETA: 8s - loss: 0.1 - ETA: 5s - loss: 0.1391 - mean_squared_error: 0.0362 - mean_absolu - ETA: 4s - loss: 0.1388 - mean_squared_error: 0.0358 - m - ETA: 3s - loss: - ETA: 0s - loss: 0.1372 - mean_squared_error: 0.0362 - mean_absolute\n",
      "Epoch 93/1000\n",
      "31210/31210 [==============================] - 16s 515us/sample - loss: 0.1379 - mean_squared_error: 0.0360 - mean_absolute_error: 0.1379 - val_loss: 0.3060 - val_mean_squared_error: 0.1980 - val_mean_absolute_error: 0.30600.1542 - mean_squared_error: 0.0422 - mean_absolute_error: 0.154 - ETA: 10s - loss: 0.1534 - mean_squared_error: 0.0418 - mean_absolute_error: 0. - ETA: 10s - loss: 0.1529 - mean_squared_error: 0.0423 - mean_absolute_error: 0.152 - ETA: 10s - loss: 0.1523 - mean_squared_error: 0.0420 - mean_absolute_error: - ETA: 9s - loss: 0.150 - ETA: 7s - loss: 0.1446 - mean_squared_error: 0.0369 - mean_absolute_error:  - ETA\n",
      "Epoch 94/1000\n",
      "31210/31210 [==============================] - 16s 515us/sample - loss: 0.1379 - mean_squared_error: 0.0363 - mean_absolute_error: 0.1379 - val_loss: 0.2973 - val_mean_squared_error: 0.1803 - val_mean_absolute_error: 0.2973 - loss: 0.1407 - mean_squared_error: 0.0351 - m - ETA: 5s - loss: 0.1417 - mean_squared_error: 0.0356 - mean_ab - ETA: 1s - loss: 0.1387 - mean_squared_error: 0.0366\n",
      "Epoch 95/1000\n",
      "31210/31210 [==============================] - 16s 515us/sample - loss: 0.1386 - mean_squared_error: 0.0363 - mean_absolute_error: 0.1386 - val_loss: 0.2975 - val_mean_squared_error: 0.1774 - val_mean_absolute_error: 0.2975ean_squared_error: 0.0351 - - ETA: 5s - loss: 0.1414 - mean_squared_error: 0.0352 - ETA: 4s - loss: 0.1424 - mean_squared_error: 0.0393 - mean_absolute_e - ETA: 3s - loss: 0.1417 - mean_squared_error: 0.0387 - mean_absolute - ETA: 3s - loss: 0.1409 - mean_squared_error: 0.0381 - mean_absolute_error: 0.14 - ETA: 2s - loss: 0.1409 - me - ETA: 0s - loss: 0.1391 - mean_squared_error: 0.0368 - mean_absolu\n",
      "Epoch 96/1000\n",
      "31210/31210 [==============================] - 16s 515us/sample - loss: 0.1392 - mean_squared_error: 0.0369 - mean_absolute_error: 0.1392 - val_loss: 0.3066 - val_mean_squared_error: 0.1904 - val_mean_absolute_error: 0.3066- mean_absolute_error - ETA: 9s - loss: 0.1500 - mean_squared_error: 0.0410 - mean_absolute_error: 0.15 - ETA: 9s - loss: 0.1494 - mean_squared_error: 0.0407 - mean_absolute_err - ETA: 5s - loss: 0.1429 - mean_squared_error: 0.0378 - mean_absolu - ETA - ETA: 1s - loss: 0.1401 - mean_squared_error: 0.0374 -\n",
      "Epoch 97/1000\n",
      "31210/31210 [==============================] - 16s 516us/sample - loss: 0.1333 - mean_squared_error: 0.0339 - mean_absolute_error: 0.1333 - val_loss: 0.3140 - val_mean_squared_error: 0.1927 - val_mean_absolute_error: 0.31400 - mean_squared_error: 0.0382 - mean_ab - ETA: 10s - loss: 0.1389 - mean_squar - ETA: 8s - loss: 0.1336 - mean_squared_error: 0.0320 - mea - ETA: 3s - loss: 0.1323 - mean_squared_error: 0.0345 - mean_absolute_error - ETA: 3s - loss: 0.1 - ETA: 0s - loss: 0.1333 - mean_squared_error: 0.0342 - mean_abso - ETA: 0s - loss: 0.1333 - mean_squared_error: 0.0339 - mean_absolute_error: 0.13\n",
      "Epoch 98/1000\n",
      "31210/31210 [==============================] - 16s 516us/sample - loss: 0.1334 - mean_squared_error: 0.0337 - mean_absolute_error: 0.1334 - val_loss: 0.3101 - val_mean_squared_error: 0.1844 - val_mean_absolute_error: 0.3101.1384 - mean_squared_error: 0.0345 - mean_absolute_error - ETA: 9s - loss: 0.1378 - mean_squared_error: 0.0340 - mean_absolute_error - ETA: 8s - loss: 0.1373 - mean_squared_error:  - ETA: 3s - loss: 0.1325 - mean_squared_error: 0.0343 - mean_ - ETA: 2s - loss: 0.1320 - mean_squared_error: 0.0337 - mean_absolute_error: 0.13 - ETA: 2s - loss: 0.1320 - mean_squared_error: 0.0337 - mean_absolute_error:  - ETA: 2s - loss: 0.1325 - mean_squared_err - ETA: 0s - loss: 0.1335 - mean_squared_error: 0.0339 - mean_absolute_e\n",
      "Epoch 99/1000\n",
      "31210/31210 [==============================] - 16s 516us/sample - loss: 0.1406 - mean_squared_error: 0.0373 - mean_absolute_error: 0.1406 - val_loss: 0.3286 - val_mean_squared_error: 0.2091 - val_mean_absolute_error: 0.3286error: 0.0373 - mean_ab\n",
      "Epoch 100/1000\n",
      "31210/31210 [==============================] - 16s 517us/sample - loss: 0.1422 - mean_squared_error: 0.0378 - mean_absolute_error: 0.1422 - val_loss: 0.3366 - val_mean_squared_error: 0.2197 - val_mean_absolute_error: 0.3366: 0.1422 - mean_squared_error: 0.0380 - m\n",
      "Epoch 101/1000\n",
      "31210/31210 [==============================] - 16s 517us/sample - loss: 0.1422 - mean_squared_error: 0.0383 - mean_absolute_error: 0.1422 - val_loss: 0.3219 - val_mean_squared_error: 0.2063 - val_mean_absolute_error: 0.3219\n",
      "Epoch 102/1000\n",
      "31210/31210 [==============================] - 16s 517us/sample - loss: 0.1405 - mean_squared_error: 0.0372 - mean_absolute_error: 0.1405 - val_loss: 0.3283 - val_mean_squared_error: 0.2257 - val_mean_absolute_error: 0.3283\n",
      "Epoch 103/1000\n",
      "31210/31210 [==============================] - 16s 517us/sample - loss: 0.1380 - mean_squared_error: 0.0362 - mean_absolute_error: 0.1380 - val_loss: 0.3162 - val_mean_squared_error: 0.2051 - val_mean_absolute_error: 0.3162\n",
      "Epoch 104/1000\n",
      "31210/31210 [==============================] - 16s 517us/sample - loss: 0.1406 - mean_squared_error: 0.0377 - mean_absolute_error: 0.1406 - val_loss: 0.3040 - val_mean_squared_error: 0.1841 - val_mean_absolute_error: 0.3040quared - ETA: 6s - loss: 0.1460 - mean_squared_error: 0.0381 - m - ETA: 5s - loss: 0.1 - ETA: 3s - loss: 0.1403 - mean_squared_error: 0.0383 - m - ETA: 1s - loss: 0.1403 - mean_squ\n",
      "Epoch 105/1000\n",
      "31210/31210 [==============================] - 16s 517us/sample - loss: 0.1344 - mean_squared_error: 0.0344 - mean_absolute_error: 0.1344 - val_loss: 0.2976 - val_mean_squared_error: 0.1828 - val_mean_absolute_error: 0.2976\n",
      "Epoch 106/1000\n",
      "31210/31210 [==============================] - 16s 517us/sample - loss: 0.1351 - mean_squared_error: 0.0348 - mean_absolute_error: 0.1351 - val_loss: 0.3160 - val_mean_squared_error: 0.2040 - val_mean_absolute_error: 0.3160\n",
      "Epoch 107/1000\n",
      "31210/31210 [==============================] - 16s 518us/sample - loss: 0.1343 - mean_squared_error: 0.0342 - mean_absolute_error: 0.1343 - val_loss: 0.3140 - val_mean_squared_error: 0.2038 - val_mean_absolute_error: 0.3140squared_error - ETA: 2s - loss: 0.1350 \n",
      "Epoch 108/1000\n",
      "31210/31210 [==============================] - 16s 517us/sample - loss: 0.1346 - mean_squared_error: 0.0346 - mean_absolute_error: 0.1346 - val_loss: 0.2966 - val_mean_squared_error: 0.1822 - val_mean_absolute_error: 0.2966- mean_ab - ETA: 1s - loss: 0.1358 - mean_squared_error: 0.0353 - mean_abso - ETA: 0s - loss: 0.1353 - mean_squared_error: 0.0352 - mean_ab\n",
      "Epoch 109/1000\n",
      "31210/31210 [==============================] - 16s 517us/sample - loss: 0.1393 - mean_squared_error: 0.0371 - mean_absolute_error: 0.1393 - val_loss: 0.2953 - val_mean_squared_error: 0.1812 - val_mean_absolute_error: 0.2953ss: 0.1473 - mean_squared_error: 0.0392 - mean_absolute_e - - ETA: 3s - loss: 0.1418 - mean_squared_error: 0.0394 - mean_ab - ETA: 2s - los\n",
      "Epoch 110/1000\n",
      "31210/31210 [==============================] - 16s 517us/sample - loss: 0.1334 - mean_squared_error: 0.0339 - mean_absolute_error: 0.1334 - val_loss: 0.3161 - val_mean_squared_error: 0.2062 - val_mean_absolute_error: 0.3161loss: 0.1368 - mean_squared_error: 0.0333 - m - ETA: 2s - loss: 0.1\n",
      "Epoch 111/1000\n",
      "31210/31210 [==============================] - 16s 517us/sample - loss: 0.1400 - mean_squared_error: 0.0369 - mean_absolute_error: 0.1400 - val_loss: 0.3166 - val_mean_squared_error: 0.2076 - val_mean_absolute_error: 0.3166- ETA: 4s - loss: 0.1438 - mean_squared_error: 0.0377 - mean_absolu - ETA: 3s - loss: 0.1438 - mean_squared_error: 0.0397 - mean_absolute - ETA: 3s - loss: 0.1435 - mean_squared_error: 0.03 - ETA: 1s - loss: 0.1423 - mean_squared_e\n",
      "Epoch 112/1000\n",
      "31210/31210 [==============================] - 16s 517us/sample - loss: 0.1390 - mean_squared_error: 0.0368 - mean_absolute_error: 0.1390 - val_loss: 0.3050 - val_mean_squared_error: 0.1909 - val_mean_absolute_error: 0.3050: 7s - loss: 0.1477 - mean_squared_error: 0.0392 - mean_absolute_error - ETA: 7s - loss: 0.1470 - mean_squ - E - ETA: 1s - loss: 0.1409 - mean_squared_error\n",
      "Epoch 113/1000\n",
      "31210/31210 [==============================] - 16s 517us/sample - loss: 0.1338 - mean_squared_error: 0.0344 - mean_absolute_error: 0.1338 - val_loss: 0.3022 - val_mean_squared_error: 0.1888 - val_mean_absolute_error: 0.3022\n",
      "Epoch 114/1000\n",
      "31210/31210 [==============================] - 16s 516us/sample - loss: 0.1383 - mean_squared_error: 0.0360 - mean_absolute_error: 0.1383 - val_loss: 0.3069 - val_mean_squared_error: 0.1904 - val_mean_absolute_error: 0.30691393 - mean_squared_error: 0.\n",
      "Epoch 115/1000\n",
      "31210/31210 [==============================] - 16s 517us/sample - loss: 0.1349 - mean_squared_error: 0.0345 - mean_absolute_error: 0.1349 - val_loss: 0.3225 - val_mean_squared_error: 0.2155 - val_mean_absolute_error: 0.3225squared_error: 0.0406 - mean_absol - ETA: 9s - loss: 0.1436 - mean_squ - ETA: 7s - loss: 0.1400 - me - ETA: 5s - loss: 0.1368 - mean_squared_error: 0.0346 - mean_absolu - ETA: 4s - loss: 0.1368 - mean_squared_error: 0.0344 - m\n",
      "Epoch 116/1000\n",
      "31210/31210 [==============================] - 16s 517us/sample - loss: 0.1295 - mean_squared_error: 0.0325 - mean_absolute_error: 0.1295 - val_loss: 0.3145 - val_mean_squared_error: 0.2053 - val_mean_absolute_error: 0.3145squared_error: \n",
      "Epoch 117/1000\n",
      "31210/31210 [==============================] - 16s 517us/sample - loss: 0.1280 - mean_squared_error: 0.0313 - mean_absolute_error: 0.1280 - val_loss: 0.3047 - val_mean_squared_error: 0.1972 - val_mean_absolute_error: 0.3047ror: 0.0318 -\n",
      "Epoch 118/1000\n",
      "31210/31210 [==============================] - 16s 517us/sample - loss: 0.1288 - mean_squared_error: 0.0316 - mean_absolute_error: 0.1288 - val_loss: 0.3065 - val_mean_squared_error: 0.1975 - val_mean_absolute_error: 0.30651296 - mean_squared\n",
      "Epoch 119/1000\n",
      "31210/31210 [==============================] - 16s 517us/sample - loss: 0.1280 - mean_squared_error: 0.0313 - mean_absolute_error: 0.1280 - val_loss: 0.3067 - val_mean_squared_error: 0.1979 - val_mean_absolute_error: 0.3067318 - mean_absolu - ETA: 2s - loss: 0.1282 - mean_squared_error: 0.0320 - mean_absolute_error: 0. - ETA: 2s - loss: 0.1284 - mean_squared_error: 0.0320 - mean_absolute_error - ETA: 1s - loss: 0.1282 - mean_squared_error: 0.0318 - mean_absolute - ETA: 1s - loss: 0.1278 - mean_squared_error: 0.\n",
      "Epoch 120/1000\n",
      "31210/31210 [==============================] - 16s 517us/sample - loss: 0.1342 - mean_squared_error: 0.0344 - mean_absolute_error: 0.1342 - val_loss: 0.3199 - val_mean_squared_error: 0.2100 - val_mean_absolute_error: 0.3199- me - ETA: 3s - loss: 0.1354 - mean_squared_error: 0.0361 - mean_absolu - ETA: 2s - loss: 0.1350 - me - ETA: 0s - loss: 0.1346 - mean_squared_error: 0.0347 - mean_absolute_e - ETA: 0s - loss: 0.1342 - mean_squared_error: 0.0344 - mean_absolute_error: 0.13\n",
      "Epoch 121/1000\n",
      "31210/31210 [==============================] - 16s 517us/sample - loss: 0.1284 - mean_squared_error: 0.0315 - mean_absolute_error: 0.1284 - val_loss: 0.3210 - val_mean_squared_error: 0.2153 - val_mean_absolute_error: 0.3210- loss: 0.1445 - mean_squared_error: 0.0383 - mean_absolute_error: 0.1 - ETA: 11s - loss: 0.1410 - mean_squa - ETA: 8s - loss: 0.1340 - mean_squared_error: 0.0321 - mean_absolute_error: 0.13 - ETA: 8s - los - ETA: 2s - loss: 0.1282 - mean_squared_error: 0.0319 - mean_absolute_error:  - ETA: 1s - loss: 0.1287 - mean_squared_error: 0.0320 - mean_abso - ETA: 1s - loss: 0.1288 - mean_squared_error: 0.0319 - mean_absolute_error: 0. - ETA: 0s - loss: 0.1290 - mean_squared_error: 0.0321 - mean_\n",
      "Epoch 122/1000\n",
      "31210/31210 [==============================] - 16s 518us/sample - loss: 0.1280 - mean_squared_error: 0.0315 - mean_absolute_error: 0.1280 - val_loss: 0.3055 - val_mean_squared_error: 0.2041 - val_mean_absolute_error: 0.3055r: 0. - ETA: 4s - loss: 0.1306 - mean_squared_error: 0.0338 - mean_absolu - ETA: 3s - loss: 0.1297 - mean_squared_error: 0.0331 - mean_absolute_error: 0. - ETA: 3s - loss: 0.1295  - ETA: 0s - loss: 0.1282 - mean_squared_error: 0.0318 - mean_absolute_error:  - ETA: 0s - loss: 0.1283 - mean_squared_error: 0.0317 - mean_absolute_err\n",
      "Epoch 123/1000\n",
      "31210/31210 [==============================] - 16s 517us/sample - loss: 0.1284 - mean_squared_error: 0.0316 - mean_absolute_error: 0.1284 - val_loss: 0.2973 - val_mean_squared_error: 0.1834 - val_mean_absolute_error: 0.2973 0.0317 - ETA: 4s - loss: 0.1307 - mean_squared_error: 0.0338 - mean_absolute_error: 0. - ETA: 3s - loss: 0.1304 - mean_squared_error: 0.0335 - mean_absolute_error:  - ETA: 3s - loss: 0.1301 - mean_squared_error: 0.0333 - mean_absolute_err - ETA: 3s - loss: 0.1299 - mean_squared_error: 0.0330 - mean_absolu - ETA: 2s - loss: 0.1295 \n",
      "Epoch 124/1000\n",
      "31210/31210 [==============================] - 16s 517us/sample - loss: 0.1330 - mean_squared_error: 0.0335 - mean_absolute_error: 0.1330 - val_loss: 0.3095 - val_mean_squared_error: 0.1993 - val_mean_absolute_error: 0.30956s - loss: 0.1297 - mean_squared_error: 0.0299 - mean_absolute_error:  - ETA:  - ETA: 3s - loss: 0.1331 - mean_squared_error: 0.0342 - mean_absolute_error: 0. - ETA: 2s - loss:\n",
      "Epoch 125/1000\n",
      "31210/31210 [==============================] - 16s 524us/sample - loss: 0.1357 - mean_squared_error: 0.0347 - mean_absolute_error: 0.1357 - val_loss: 0.3464 - val_mean_squared_error: 0.2489 - val_mean_absolute_error: 0.3464- loss: 0.1397 - - ETA - ETA: 0s - loss: 0.1358 - mean_squared_error: 0.0348 - mean_absolute_error: 0.\n",
      "Epoch 126/1000\n",
      "31210/31210 [==============================] - 16s 514us/sample - loss: 0.1317 - mean_squared_error: 0.0334 - mean_absolute_error: 0.1317 - val_loss: 0.3027 - val_mean_squared_error: 0.1959 - val_mean_absolute_error: 0.3027 - loss: 0.1510 - mean_squared_erro - ETA: 9s - loss: 0.1351 - mean_squared_err - ETA: 7s - loss: 0.1340 - mean_squared_error: 0.0324 - mean_absolute_e - ETA: 7s - loss: 0.1328 - mean_squared_error: 0.0317 - ETA: 5s - loss: 0.1306 - mean_squared_error: 0.0307 - - ETA: 4s - loss: 0.1316 - mean_squared_error: 0.0323 - mean_absolute_error: 0.13 - ETA: 4s - loss: 0.1314 - mean_squared_err - ETA: 2s - loss: 0\n",
      "Epoch 127/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1269 - mean_squared_error: 0.0310 - mean_absolute_error: 0.1269 - val_loss: 0.2945 - val_mean_squared_error: 0.1781 - val_mean_absolute_error: 0.2945\n",
      "Epoch 128/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1263 - mean_squared_error: 0.0304 - mean_absolute_error: 0.1263 - val_loss: 0.3154 - val_mean_squared_error: 0.1969 - val_mean_absolute_error: 0.3154\n",
      "Epoch 129/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1294 - mean_squared_error: 0.0323 - mean_absolute_error: 0.1294 - val_loss: 0.3469 - val_mean_squared_error: 0.2583 - val_mean_absolute_error: 0.3469\n",
      "Epoch 130/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1323 - mean_squared_error: 0.0333 - mean_absolute_error: 0.1323 - val_loss: 0.3184 - val_mean_squared_error: 0.2071 - val_mean_absolute_error: 0.3184\n",
      "Epoch 131/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1341 - mean_squared_error: 0.0344 - mean_absolute_error: 0.1341 - val_loss: 0.3197 - val_mean_squared_error: 0.2044 - val_mean_absolute_error: 0.3197s: 0.1422 - mean_squared_error: 0.0363 - mean_absolute_e - ETA: 5s - loss: 0.1405 - mean_squ\n",
      "Epoch 132/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1287 - mean_squared_error: 0.0315 - mean_absolute_error: 0.1287 - val_loss: 0.3519 - val_mean_squared_error: 0.2454 - val_mean_absolute_error: 0.3519: 0.1305 - mean_squared_error: 0.0331 - mean_absolute_error:  - ETA: 2s - los\n",
      "Epoch 133/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1287 - mean_squared_error: 0.0316 - mean_absolute_error: 0.1287 - val_loss: 0.3387 - val_mean_squared_error: 0.2359 - val_mean_absolute_error: 0.3387- loss: 0.1291 - mean_squared_error: 0.0319 - mean_absolute_e\n",
      "Epoch 134/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1295 - mean_squared_error: 0.0319 - mean_absolute_error: 0.1295 - val_loss: 0.3039 - val_mean_squared_error: 0.1955 - val_mean_absolute_error: 0.3039- mean_squared_error: 0.0330 - ETA: 1s - loss: 0.1303 - mean_squared_error: 0.0324 - mean_absolute_error:  - ETA: 1s - loss: 0.1302 - mean_squared_error: 0.0323 - mean_absolute_e - ETA: 0s - loss: 0.1303 - mean_squared_error: 0.0325 - mean_ab\n",
      "Epoch 135/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1266 - mean_squared_error: 0.0304 - mean_absolute_error: 0.1266 - val_loss: 0.2923 - val_mean_squared_error: 0.1768 - val_mean_absolute_error: 0.2923 0.0288 - mean_absolu - ETA: 6s - loss: 0.1281 - mean_squared_error: 0.0291 - - ETA: 2s - loss: 0.1263 - mean_squared_error:  - ETA: 0s - loss: 0.1265 - mean_squared_error: 0.0305 - mean_absolute\n",
      "Epoch 136/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1286 - mean_squared_error: 0.0315 - mean_absolute_error: 0.1286 - val_loss: 0.3422 - val_mean_squared_error: 0.2330 - val_mean_absolute_error: 0.3422\n",
      "Epoch 137/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1314 - mean_squared_error: 0.0331 - mean_absolute_error: 0.1314 - val_loss: 0.3282 - val_mean_squared_error: 0.2211 - val_mean_absolute_error: 0.3282error: 0.0337 - mean_abso - ETA: 0s - loss: 0.1324 - mean_squared_error: 0.0338 - mean_absolute_error:  - ETA: 0s - loss: 0.1320 - mean_squared_error: 0.0335 - mean_absolute\n",
      "Epoch 138/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1319 - mean_squared_error: 0.0332 - mean_absolute_error: 0.1319 - val_loss: 0.3063 - val_mean_squared_error: 0.1960 - val_mean_absolute_error: 0.3063\n",
      "Epoch 139/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1271 - mean_squared_error: 0.0306 - mean_absolute_error: 0.1271 - val_loss: 0.3282 - val_mean_squared_error: 0.2241 - val_mean_absolute_error: 0.3282\n",
      "Epoch 140/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1300 - mean_squared_error: 0.0324 - mean_absolute_error: 0.1300 - val_loss: 0.3177 - val_mean_squared_error: 0.2140 - val_mean_absolute_error: 0.3177red_error: 0.0301 - mean_absolute_err - ETA: 5s - loss: 0.1 - ETA: 2s - loss:\n",
      "Epoch 141/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1280 - mean_squared_error: 0.0317 - mean_absolute_error: 0.1280 - val_loss: 0.2963 - val_mean_squared_error: 0.1898 - val_mean_absolute_error: 0.2963\n",
      "Epoch 142/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1282 - mean_squared_error: 0.0317 - mean_absolute_error: 0.1282 - val_loss: 0.2891 - val_mean_squared_error: 0.1763 - val_mean_absolute_error: 0.2891n\n",
      "Epoch 143/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1283 - mean_squared_error: 0.0314 - mean_absolute_error: 0.1283 - val_loss: 0.2954 - val_mean_squared_error: 0.1820 - val_mean_absolute_error: 0.2954\n",
      "Epoch 144/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1267 - mean_squared_error: 0.0305 - mean_absolute_error: 0.1267 - val_loss: 0.3108 - val_mean_squared_error: 0.1959 - val_mean_absolute_error: 0.3108\n",
      "Epoch 145/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1319 - mean_squared_error: 0.0326 - mean_absolute_error: 0.1319 - val_loss: 0.3382 - val_mean_squared_error: 0.2308 - val_mean_absolute_error: 0.3382oss: 0.1435 - mean_squar - ETA: 1s - loss: 0.1305 - mean_squar\n",
      "Epoch 146/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1383 - mean_squared_error: 0.0355 - mean_absolute_error: 0.1383 - val_loss: 0.3173 - val_mean_squared_error: 0.2131 - val_mean_absolute_error: 0.317389 - mean_squared_error: 0.0401  - ETA: 9s - ETA: 2s - loss: 0.1379 - mean\n",
      "Epoch 147/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1392 - mean_squared_error: 0.0362 - mean_absolute_error: 0.1392 - val_loss: 0.3000 - val_mean_squared_error: 0.1929 - val_mean_absolute_error: 0.3000\n",
      "Epoch 148/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1352 - mean_squared_error: 0.0344 - mean_absolute_error: 0.1352 - val_loss: 0.2904 - val_mean_squared_error: 0.1783 - val_mean_absolute_error: 0.29040.1\n",
      "Epoch 149/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1252 - mean_squared_error: 0.0301 - mean_absolute_error: 0.1252 - val_loss: 0.3052 - val_mean_squared_error: 0.1896 - val_mean_absolute_error: 0.30520.1254 - mean_squared_err - ETA: 1s - loss: 0.1253 - mean_squared_error: 0.0305 - mean_absolute - ETA: 0s - loss: 0.1254 - mean_squared_error: 0.0305 - mean_abso\n",
      "Epoch 150/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1252 - mean_squared_error: 0.0300 - mean_absolute_error: 0.1252 - val_loss: 0.3294 - val_mean_squared_error: 0.2191 - val_mean_absolute_error: 0.3294red_error: 0.0320 - - ETA: 9s - loss: 0.1272 - mean_squared_error: 0.0299 - mean_ - ETA: 8s - loss: 0.1268 - mean_squ - ETA: 6s - loss: 0.124\n",
      "Epoch 151/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1249 - mean_squared_error: 0.0297 - mean_absolute_error: 0.1249 - val_loss: 0.3147 - val_mean_squared_error: 0.1962 - val_mean_absolute_error: 0.3147\n",
      "Epoch 152/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1233 - mean_squared_error: 0.0293 - mean_absolute_error: 0.1233 - val_loss: 0.3288 - val_mean_squared_error: 0.2212 - val_mean_absolute_error: 0.3288\n",
      "Epoch 153/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1262 - mean_squared_error: 0.0306 - mean_absolute_error: 0.1262 - val_loss: 0.2941 - val_mean_squared_error: 0.1878 - val_mean_absolute_error: 0.2941\n",
      "Epoch 154/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1302 - mean_squared_error: 0.0322 - mean_absolute_error: 0.1302 - val_loss: 0.2993 - val_mean_squared_error: 0.1928 - val_mean_absolute_error: 0.2993\n",
      "Epoch 155/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1241 - mean_squared_error: 0.0295 - mean_absolute_error: 0.1241 - val_loss: 0.3195 - val_mean_squared_error: 0.2152 - val_mean_absolute_error: 0.3195\n",
      "Epoch 156/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1241 - mean_squared_error: 0.0295 - mean_absolute_error: 0.1241 - val_loss: 0.3175 - val_mean_squared_error: 0.2066 - val_mean_absolute_error: 0.3175\n",
      "Epoch 157/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1260 - mean_squared_error: 0.0302 - mean_absolute_error: 0.1260 - val_loss: 0.3058 - val_mean_squared_error: 0.1901 - val_mean_absolute_error: 0.3058squared_error - ETA: 4s - loss: 0.1276 - mean - ETA: 1s - loss: 0.1265 - mean_squared\n",
      "Epoch 158/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1272 - mean_squared_error: 0.0309 - mean_absolute_error: 0.1272 - val_loss: 0.3200 - val_mean_squared_error: 0.2109 - val_mean_absolute_error: 0.3200ss: 0.1276 - mean_squared_error: 0.0312 - mean_absolu\n",
      "Epoch 159/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1215 - mean_squared_error: 0.0286 - mean_absolute_error: 0.1215 - val_loss: 0.3136 - val_mean_squared_error: 0.2116 - val_mean_absolute_error: 0.3136- loss: 0.1231 - mean_squared_error: 0.0305 - mean_absolute_e\n",
      "Epoch 160/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1206 - mean_squared_error: 0.0281 - mean_absolute_error: 0.1206 - val_loss: 0.3001 - val_mean_squared_error: 0.1960 - val_mean_absolute_error: 0.3001\n",
      "Epoch 161/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1180 - mean_squared_error: 0.0271 - mean_absolute_error: 0.1180 - val_loss: 0.3138 - val_mean_squared_error: 0.2064 - val_mean_absolute_error: 0.3138\n",
      "Epoch 162/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1176 - mean_squared_error: 0.0269 - mean_absolute_error: 0.1176 - val_loss: 0.3124 - val_mean_squared_error: 0.2075 - val_mean_absolute_error: 0.3124\n",
      "Epoch 163/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1195 - mean_squared_error: 0.0275 - mean_absolute_error: 0.1195 - val_loss: 0.3338 - val_mean_squared_error: 0.2280 - val_mean_absolute_error: 0.3338\n",
      "Epoch 164/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1223 - mean_squared_error: 0.0290 - mean_absolute_error: 0.1223 - val_loss: 0.3180 - val_mean_squared_error: 0.2128 - val_mean_absolute_error: 0.3180\n",
      "Epoch 165/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1221 - mean_squared_error: 0.0285 - mean_absolute_error: 0.1221 - val_loss: 0.2939 - val_mean_squared_error: 0.1821 - val_mean_absolute_error: 0.2939\n",
      "Epoch 166/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1236 - mean_squared_error: 0.0290 - mean_absolute_error: 0.1236 - val_loss: 0.2979 - val_mean_squared_error: 0.1879 - val_mean_absolute_error: 0.2979uared_error: 0.0290 - mean_absolute_er - ETA: 9s - loss: 0.1259 - mean_squared_error: 0.0286 - mean_absolute_error:  - ETA: 9s - loss: 0.1261 - mean_squared_error: 0. - ETA: 0s - loss: 0.1241 - mean_squared_error: 0.0293 - mean_absolute_error:  - ETA: 0s - loss: 0.1238 - mean_squared_error: 0.0291 - mean_absolute_error: 0.\n",
      "Epoch 167/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1235 - mean_squared_error: 0.0292 - mean_absolute_error: 0.1235 - val_loss: 0.3086 - val_mean_squared_error: 0.1983 - val_mean_absolute_error: 0.3086\n",
      "Epoch 168/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1195 - mean_squared_error: 0.0275 - mean_absolute_error: 0.1195 - val_loss: 0.3437 - val_mean_squared_error: 0.2451 - val_mean_absolute_error: 0.3437\n",
      "Epoch 169/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1215 - mean_squared_error: 0.0286 - mean_absolute_error: 0.1215 - val_loss: 0.3115 - val_mean_squared_error: 0.2094 - val_mean_absolute_error: 0.3115\n",
      "Epoch 170/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1237 - mean_squared_error: 0.0293 - mean_absolute_error: 0.1237 - val_loss: 0.2990 - val_mean_squared_error: 0.1946 - val_mean_absolute_error: 0.2990\n",
      "Epoch 171/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1229 - mean_squared_error: 0.0288 - mean_absolute_error: 0.1229 - val_loss: 0.3054 - val_mean_squared_error: 0.1947 - val_mean_absolute_error: 0.3054\n",
      "Epoch 172/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1246 - mean_squared_error: 0.0296 - mean_absolute_error: 0.1246 - val_loss: 0.3083 - val_mean_squared_error: 0.1989 - val_mean_absolute_error: 0.3083mean_absolute_error\n",
      "Epoch 173/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1210 - mean_squared_error: 0.0281 - mean_absolute_error: 0.1210 - val_loss: 0.3099 - val_mean_squared_error: 0.1967 - val_mean_absolute_error: 0.3099\n",
      "Epoch 174/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1209 - mean_squared_error: 0.0281 - mean_absolute_error: 0.1209 - val_loss: 0.3091 - val_mean_squared_error: 0.1978 - val_mean_absolute_error: 0.3091ETA: 0s - loss: 0.1211 - mean_squared_error: 0.0284 - mea\n",
      "Epoch 175/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1234 - mean_squared_error: 0.0293 - mean_absolute_error: 0.1234 - val_loss: 0.3312 - val_mean_squared_error: 0.2294 - val_mean_absolute_error: 0.3312 - mean\n",
      "Epoch 176/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1222 - mean_squared_error: 0.0286 - mean_absolute_error: 0.1222 - val_loss: 0.3124 - val_mean_squared_error: 0.2179 - val_mean_absolute_error: 0.3124\n",
      "Epoch 177/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1201 - mean_squared_error: 0.0277 - mean_absolute_error: 0.1201 - val_loss: 0.2940 - val_mean_squared_error: 0.1920 - val_mean_absolute_error: 0.2940\n",
      "Epoch 178/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1185 - mean_squared_error: 0.0272 - mean_absolute_error: 0.1185 - val_loss: 0.2934 - val_mean_squared_error: 0.1878 - val_mean_absolute_error: 0.2934\n",
      "Epoch 179/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1174 - mean_squared_error: 0.0268 - mean_absolute_error: 0.1174 - val_loss: 0.2930 - val_mean_squared_error: 0.1811 - val_mean_absolute_error: 0.2930\n",
      "Epoch 180/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1163 - mean_squared_error: 0.0262 - mean_absolute_error: 0.1163 - val_loss: 0.3091 - val_mean_squared_error: 0.1997 - val_mean_absolute_error: 0.3091 - ETA: 0s - loss: 0.1165 - mean_squared_error: 0.0264 - mean_absolute_err\n",
      "Epoch 181/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1173 - mean_squared_error: 0.0268 - mean_absolute_error: 0.1173 - val_loss: 0.3369 - val_mean_squared_error: 0.2392 - val_mean_absolute_error: 0.3369\n",
      "Epoch 182/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1149 - mean_squared_error: 0.0258 - mean_absolute_error: 0.1149 - val_loss: 0.3287 - val_mean_squared_error: 0.2273 - val_mean_absolute_error: 0.3287 loss: 0.1231 - mean_squared_error: 0.0274 - mea - ETA: 1s - loss: 0.1163 - mean_squared_e\n",
      "Epoch 183/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1211 - mean_squared_error: 0.0285 - mean_absolute_error: 0.1211 - val_loss: 0.3013 - val_mean_squared_error: 0.1971 - val_mean_absolute_error: 0.3013\n",
      "Epoch 184/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1276 - mean_squared_error: 0.0310 - mean_absolute_error: 0.1276 - val_loss: 0.2778 - val_mean_squared_error: 0.1713 - val_mean_absolute_error: 0.2778\n",
      "Epoch 185/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1301 - mean_squared_error: 0.0323 - mean_absolute_error: 0.1301 - val_loss: 0.3231 - val_mean_squared_error: 0.2291 - val_mean_absolute_error: 0.3231\n",
      "Epoch 186/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1196 - mean_squared_error: 0.0280 - mean_absolute_error: 0.1196 - val_loss: 0.3072 - val_mean_squared_error: 0.2041 - val_mean_absolute_error: 0.30721346 - mean_squared_error: 0.0339 - mean_absolute_error: 0.1 - ETA: 11s - loss: 0. - ETA: 7s - loss: 0.1237 - mean - ETA: 5s - loss: 0.1210 - mean_squared_err\n",
      "Epoch 187/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1197 - mean_squared_error: 0.0277 - mean_absolute_error: 0.1197 - val_loss: 0.2994 - val_mean_squared_error: 0.1937 - val_mean_absolute_error: 0.2994\n",
      "Epoch 188/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1204 - mean_squared_error: 0.0280 - mean_absolute_error: 0.1204 - val_loss: 0.3171 - val_mean_squared_error: 0.2135 - val_mean_absolute_error: 0.317102\n",
      "Epoch 189/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1234 - mean_squared_error: 0.0294 - mean_absolute_error: 0.1234 - val_loss: 0.3174 - val_mean_squared_error: 0.2182 - val_mean_absolute_error: 0.3174ed_error: 0.0285 - ETA: 5s - loss: 0.1 - ETA: 2s - loss:\n",
      "Epoch 190/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1218 - mean_squared_error: 0.0285 - mean_absolute_error: 0.1218 - val_loss: 0.3113 - val_mean_squared_error: 0.2072 - val_mean_absolute_error: 0.31131215 - mean_squared_error\n",
      "Epoch 191/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1227 - mean_squared_error: 0.0292 - mean_absolute_error: 0.1227 - val_loss: 0.3180 - val_mean_squared_error: 0.2164 - val_mean_absolute_error: 0.3180- ETA: 2s - loss: 0.1227 - mean - ETA: 0s - loss: 0.1227 - mean_squared_error: 0.0292 - mean_absolute_error: 0.12\n",
      "Epoch 192/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1136 - mean_squared_error: 0.0251 - mean_absolute_error: 0.1136 - val_loss: 0.3222 - val_mean_squared_error: 0.2204 - val_mean_absolute_error: 0.3222\n",
      "Epoch 193/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1133 - mean_squared_error: 0.0251 - mean_absolute_error: 0.1133 - val_loss: 0.3223 - val_mean_squared_error: 0.2179 - val_mean_absolute_error: 0.3223\n",
      "Epoch 194/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1181 - mean_squared_error: 0.0271 - mean_absolute_error: 0.1181 - val_loss: 0.2925 - val_mean_squared_error: 0.1844 - val_mean_absolute_error: 0.2925\n",
      "Epoch 195/1000\n",
      "31210/31210 [==============================] - 16s 503us/sample - loss: 0.1198 - mean_squared_error: 0.0275 - mean_absolute_error: 0.1198 - val_loss: 0.2979 - val_mean_squared_error: 0.1915 - val_mean_absolute_error: 0.2979\n",
      "Epoch 196/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1171 - mean_squared_error: 0.0266 - mean_absolute_error: 0.1171 - val_loss: 0.3390 - val_mean_squared_error: 0.2499 - val_mean_absolute_error: 0.3390\n",
      "Epoch 197/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1155 - mean_squared_error: 0.0261 - mean_absolute_error: 0.1155 - val_loss: 0.3084 - val_mean_squared_error: 0.2029 - val_mean_absolute_error: 0.3084n_squared_error: 0.0262 - mean_absolute_err\n",
      "Epoch 198/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1206 - mean_squared_error: 0.0279 - mean_absolute_error: 0.1206 - val_loss: 0.2951 - val_mean_squared_error: 0.1874 - val_mean_absolute_error: 0.2951\n",
      "Epoch 199/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1210 - mean_squared_error: 0.0283 - mean_absolute_error: 0.1210 - val_loss: 0.3330 - val_mean_squared_error: 0.2296 - val_mean_absolute_error: 0.3330\n",
      "Epoch 200/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1234 - mean_squared_error: 0.0291 - mean_absolute_error: 0.1234 - val_loss: 0.3140 - val_mean_squared_error: 0.2165 - val_mean_absolute_error: 0.3140\n",
      "Epoch 201/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1194 - mean_squared_error: 0.0281 - mean_absolute_error: 0.1194 - val_loss: 0.3066 - val_mean_squared_error: 0.2021 - val_mean_absolute_error: 0.3066\n",
      "Epoch 202/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1115 - mean_squared_error: 0.0244 - mean_absolute_error: 0.1115 - val_loss: 0.2957 - val_mean_squared_error: 0.1849 - val_mean_absolute_error: 0.2957\n",
      "Epoch 203/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1079 - mean_squared_error: 0.0230 - mean_absolute_error: 0.1079 - val_loss: 0.3154 - val_mean_squared_error: 0.2034 - val_mean_absolute_error: 0.3154\n",
      "Epoch 204/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1156 - mean_squared_error: 0.0262 - mean_absolute_error: 0.1156 - val_loss: 0.3258 - val_mean_squared_error: 0.2191 - val_mean_absolute_error: 0.3258\n",
      "Epoch 205/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1164 - mean_squared_error: 0.0260 - mean_absolute_error: 0.1164 - val_loss: 0.3044 - val_mean_squared_error: 0.2027 - val_mean_absolute_error: 0.3044\n",
      "Epoch 206/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1148 - mean_squared_error: 0.0255 - mean_absolute_error: 0.1148 - val_loss: 0.2865 - val_mean_squared_error: 0.1758 - val_mean_absolute_error: 0.2865\n",
      "Epoch 207/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1200 - mean_squared_error: 0.0274 - mean_absolute_error: 0.1200 - val_loss: 0.2984 - val_mean_squared_error: 0.1848 - val_mean_absolute_error: 0.2984\n",
      "Epoch 208/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1173 - mean_squared_error: 0.0264 - mean_absolute_error: 0.1173 - val_loss: 0.3460 - val_mean_squared_error: 0.2460 - val_mean_absolute_error: 0.3460error: 0.0266 - mean_abso\n",
      "Epoch 209/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1179 - mean_squared_error: 0.0267 - mean_absolute_error: 0.1179 - val_loss: 0.3197 - val_mean_squared_error: 0.2188 - val_mean_absolute_error: 0.3197\n",
      "Epoch 210/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1201 - mean_squared_error: 0.0276 - mean_absolute_error: 0.1201 - val_loss: 0.2990 - val_mean_squared_error: 0.1921 - val_mean_absolute_error: 0.2990271 - mean_squared_error: 0.0290 - mean_ - - ETA: 1s - loss: 0.1206 - mean_squared_error: 0.0280 - m\n",
      "Epoch 211/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1209 - mean_squared_error: 0.0279 - mean_absolute_error: 0.1209 - val_loss: 0.3155 - val_mean_squared_error: 0.2110 - val_mean_absolute_error: 0.3155oss: 0.1204 - mean_squared - ETA: 0s - loss: 0.1211 - mean_squared_error: 0.0282 - mean_\n",
      "Epoch 212/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1212 - mean_squared_error: 0.0282 - mean_absolute_error: 0.1212 - val_loss: 0.3168 - val_mean_squared_error: 0.2154 - val_mean_absolute_error: 0.3168\n",
      "Epoch 213/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1200 - mean_squared_error: 0.0278 - mean_absolute_error: 0.1200 - val_loss: 0.3061 - val_mean_squared_error: 0.2007 - val_mean_absolute_error: 0.3061error: 0.0286 - mean_absolute_error:  - ETA: 2s - loss: 0.1205 - mean_squared_error: 0.0285 - mean_ - ETA: 1s - loss: 0.1203 - mean_squared_error: 0.0282 -\n",
      "Epoch 214/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1136 - mean_squared_error: 0.0252 - mean_absolute_error: 0.1136 - val_loss: 0.2958 - val_mean_squared_error: 0.1891 - val_mean_absolute_error: 0.2958\n",
      "Epoch 215/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1124 - mean_squared_error: 0.0250 - mean_absolute_error: 0.1124 - val_loss: 0.3105 - val_mean_squared_error: 0.2108 - val_mean_absolute_error: 0.3105 3s - loss: 0.1138 - mean_squ - ETA: 1s - loss: 0.1131 - mean_squared_e\n",
      "Epoch 216/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1173 - mean_squared_error: 0.0267 - mean_absolute_error: 0.1173 - val_loss: 0.3004 - val_mean_squared_error: 0.1925 - val_mean_absolute_error: 0.3004loss: 0.1380 - mean_squ - ETA: 5s - loss: 0.1195 - mean - ETA: 3s - loss: 0.1190 - mean_squared_error: 0.0283 - ETA: 2s - loss: 0.1182 - mean_s\n",
      "Epoch 217/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1127 - mean_squared_error: 0.0248 - mean_absolute_error: 0.1127 - val_loss: 0.2953 - val_mean_squared_error: 0.1904 - val_mean_absolute_error: 0.2953\n",
      "Epoch 218/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1126 - mean_squared_error: 0.0247 - mean_absolute_error: 0.1126 - val_loss: 0.3179 - val_mean_squared_error: 0.2138 - val_mean_absolute_error: 0.3179\n",
      "Epoch 219/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1124 - mean_squared_error: 0.0246 - mean_absolute_error: 0.1124 - val_loss: 0.3274 - val_mean_squared_error: 0.2240 - val_mean_absolute_error: 0.3274\n",
      "Epoch 220/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1145 - mean_squared_error: 0.0252 - mean_absolute_error: 0.1145 - val_loss: 0.3013 - val_mean_squared_error: 0.1873 - val_mean_absolute_error: 0.3013\n",
      "Epoch 221/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1166 - mean_squared_error: 0.0262 - mean_absolute_error: 0.1166 - val_loss: 0.3386 - val_mean_squared_error: 0.2316 - val_mean_absolute_error: 0.3386\n",
      "Epoch 222/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1203 - mean_squared_error: 0.0278 - mean_absolute_error: 0.1203 - val_loss: 0.3347 - val_mean_squared_error: 0.2286 - val_mean_absolute_error: 0.3347\n",
      "Epoch 223/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1198 - mean_squared_error: 0.0282 - mean_absolute_error: 0.1198 - val_loss: 0.3129 - val_mean_squared_error: 0.2146 - val_mean_absolute_error: 0.3129\n",
      "Epoch 224/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1095 - mean_squared_error: 0.0237 - mean_absolute_error: 0.1095 - val_loss: 0.2972 - val_mean_squared_error: 0.1922 - val_mean_absolute_error: 0.2972 loss: 0.1296 - mean_squared_error: 0.0319 - m - ETA: 10s - loss: 0.1217 - mean_squared_error: 0.0269 - m - ETA: 8s - loss: 0.1169 - mean_squared_err - ETA:  - ETA: 3s - loss: 0.1108 - mean_squar - ETA: 1s - loss: 0.1099 - mean_squared\n",
      "Epoch 225/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1085 - mean_squared_error: 0.0231 - mean_absolute_error: 0.1085 - val_loss: 0.3181 - val_mean_squared_error: 0.2107 - val_mean_absolute_error: 0.3181\n",
      "Epoch 226/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1100 - mean_squared_error: 0.0237 - mean_absolute_error: 0.1100 - val_loss: 0.3151 - val_mean_squared_error: 0.2103 - val_mean_absolute_error: 0.3151\n",
      "Epoch 227/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1162 - mean_squared_error: 0.0260 - mean_absolute_error: 0.1162 - val_loss: 0.3003 - val_mean_squared_error: 0.1901 - val_mean_absolute_error: 0.3003\n",
      "Epoch 228/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1134 - mean_squared_error: 0.0249 - mean_absolute_error: 0.1134 - val_loss: 0.3165 - val_mean_squared_error: 0.2150 - val_mean_absolute_error: 0.3165\n",
      "Epoch 229/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1148 - mean_squared_error: 0.0255 - mean_absolute_error: 0.1148 - val_loss: 0.3209 - val_mean_squared_error: 0.2264 - val_mean_absolute_error: 0.3209\n",
      "Epoch 230/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1138 - mean_squared_error: 0.0252 - mean_absolute_error: 0.1138 - val_loss: 0.2851 - val_mean_squared_error: 0.1789 - val_mean_absolute_error: 0.28513 - mean_absolute\n",
      "Epoch 231/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1133 - mean_squared_error: 0.0250 - mean_absolute_error: 0.1133 - val_loss: 0.2976 - val_mean_squared_error: 0.1937 - val_mean_absolute_error: 0.2976 0.02 - ETA: 3s - loss: 0.1146 - mean_squared_error: 0.0267 - mean_ab - ETA: 2s - loss: 0.1149 - mean_squared_error: 0.0264 - mean_absolute_err - ETA: 2s - loss: 0.1148 \n",
      "Epoch 232/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1146 - mean_squared_error: 0.0255 - mean_absolute_error: 0.1146 - val_loss: 0.3111 - val_mean_squared_error: 0.1995 - val_mean_absolute_error: 0.3111\n",
      "Epoch 233/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1143 - mean_squared_error: 0.0251 - mean_absolute_error: 0.1143 - val_loss: 0.3332 - val_mean_squared_error: 0.2256 - val_mean_absolute_error: 0.3332\n",
      "Epoch 234/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1121 - mean_squared_error: 0.0246 - mean_absolute_error: 0.1121 - val_loss: 0.3308 - val_mean_squared_error: 0.2339 - val_mean_absolute_error: 0.3308 mean_squared_error: 0.0261 - mean_absolute_error:  - ETA: 3s - loss: 0.1133 - mean_squared_e - ETA: 1s - loss: 0.1127 - mean_squared_error: 0.0251 -\n",
      "Epoch 235/1000\n",
      "31210/31210 [==============================] - 16s 504us/sample - loss: 0.1133 - mean_squared_error: 0.0248 - mean_absolute_error: 0.1133 - val_loss: 0.3017 - val_mean_squared_error: 0.1978 - val_mean_absolute_error: 0.30171 - mean_absolute - ETA: 0s - loss: 0.1133 - mean_squared_error: 0.0248 - mean_absolute_error: 0.11\n",
      "Epoch 236/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1139 - mean_squared_error: 0.0250 - mean_absolute_error: 0.1139 - val_loss: 0.3204 - val_mean_squared_error: 0.2240 - val_mean_absolute_error: 0.3204\n",
      "Epoch 237/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1163 - mean_squared_error: 0.0263 - mean_absolute_error: 0.1163 - val_loss: 0.3338 - val_mean_squared_error: 0.2407 - val_mean_absolute_error: 0.3338\n",
      "Epoch 238/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1142 - mean_squared_error: 0.0252 - mean_absolute_error: 0.1142 - val_loss: 0.3099 - val_mean_squared_error: 0.2013 - val_mean_absolute_error: 0.3099d_error: 0.0254 - mean_\n",
      "Epoch 239/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1127 - mean_squared_error: 0.0246 - mean_absolute_error: 0.1127 - val_loss: 0.3052 - val_mean_squared_error: 0.1940 - val_mean_absolute_error: 0.3052\n",
      "Epoch 240/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1134 - mean_squared_error: 0.0249 - mean_absolute_error: 0.1134 - val_loss: 0.3359 - val_mean_squared_error: 0.2367 - val_mean_absolute_error: 0.3359\n",
      "Epoch 241/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1161 - mean_squared_error: 0.0259 - mean_absolute_error: 0.1161 - val_loss: 0.3444 - val_mean_squared_error: 0.2537 - val_mean_absolute_error: 0.3444\n",
      "Epoch 242/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1148 - mean_squared_error: 0.0256 - mean_absolute_error: 0.1148 - val_loss: 0.3169 - val_mean_squared_error: 0.2137 - val_mean_absolute_error: 0.3169\n",
      "Epoch 243/1000\n",
      "31210/31210 [==============================] - 16s 508us/sample - loss: 0.1146 - mean_squared_error: 0.0256 - mean_absolute_error: 0.1146 - val_loss: 0.3197 - val_mean_squared_error: 0.2193 - val_mean_absolute_error: 0.3197\n",
      "Epoch 244/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1141 - mean_squared_error: 0.0251 - mean_absolute_error: 0.1141 - val_loss: 0.3266 - val_mean_squared_error: 0.2174 - val_mean_absolute_error: 0.3266\n",
      "Epoch 245/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1170 - mean_squared_error: 0.0262 - mean_absolute_error: 0.1170 - val_loss: 0.3137 - val_mean_squared_error: 0.2023 - val_mean_absolute_error: 0.3137\n",
      "Epoch 246/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1198 - mean_squared_error: 0.0272 - mean_absolute_error: 0.1198 - val_loss: 0.3343 - val_mean_squared_error: 0.2288 - val_mean_absolute_error: 0.3343\n",
      "Epoch 247/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1226 - mean_squared_error: 0.0287 - mean_absolute_error: 0.1226 - val_loss: 0.3327 - val_mean_squared_error: 0.2301 - val_mean_absolute_error: 0.33271560 - mean_squared_error: 0.0447 - mean_absolute_erro - ETA: 11s - - ETA: 4s - loss: 0.1269 - mean_squared_error: 0.0318 - mean_absolute_e - ETA: 3s - loss: 0.1261 -  - ETA: 1s - loss: 0.1238 - mean_squared_error: 0.0294 - m\n",
      "Epoch 248/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1192 - mean_squared_error: 0.0276 - mean_absolute_error: 0.1192 - val_loss: 0.3256 - val_mean_squared_error: 0.2304 - val_mean_absolute_error: 0.3256\n",
      "Epoch 249/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1151 - mean_squared_error: 0.0256 - mean_absolute_error: 0.1151 - val_loss: 0.3419 - val_mean_squared_error: 0.2495 - val_mean_absolute_error: 0.3419\n",
      "Epoch 250/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1126 - mean_squared_error: 0.0245 - mean_absolute_error: 0.1126 - val_loss: 0.3299 - val_mean_squared_error: 0.2231 - val_mean_absolute_error: 0.3299\n",
      "Epoch 251/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1126 - mean_squared_error: 0.0248 - mean_absolute_error: 0.1126 - val_loss: 0.3263 - val_mean_squared_error: 0.2204 - val_mean_absolute_error: 0.3263error: 0.11\n",
      "Epoch 252/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1147 - mean_squared_error: 0.0254 - mean_absolute_error: 0.1147 - val_loss: 0.3254 - val_mean_squared_error: 0.2278 - val_mean_absolute_error: 0.3254\n",
      "Epoch 253/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1143 - mean_squared_error: 0.0256 - mean_absolute_error: 0.1143 - val_loss: 0.3308 - val_mean_squared_error: 0.2361 - val_mean_absolute_error: 0.3308\n",
      "Epoch 254/1000\n",
      "31210/31210 [==============================] - 16s 508us/sample - loss: 0.1173 - mean_squared_error: 0.0268 - mean_absolute_error: 0.1173 - val_loss: 0.3168 - val_mean_squared_error: 0.2130 - val_mean_absolute_error: 0.3168 0.1296 - mean_squared_error: 0.0309 - mean_absolu - ETA: 9s - loss: 0.1254 - mean_squared - ETA: 7s - loss: 0.1231 - mean - ETA: 5s - loss: - ETA: 2s - loss: 0.118\n",
      "Epoch 255/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1152 - mean_squared_error: 0.0256 - mean_absolute_error: 0.1152 - val_loss: 0.3173 - val_mean_squared_error: 0.2084 - val_mean_absolute_error: 0.3173\n",
      "Epoch 256/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1158 - mean_squared_error: 0.0258 - mean_absolute_error: 0.1158 - val_loss: 0.3206 - val_mean_squared_error: 0.2200 - val_mean_absolute_error: 0.3206\n",
      "Epoch 257/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1191 - mean_squared_error: 0.0270 - mean_absolute_error: 0.1191 - val_loss: 0.3273 - val_mean_squared_error: 0.2339 - val_mean_absolute_error: 0.3273\n",
      "Epoch 258/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1191 - mean_squared_error: 0.0275 - mean_absolute_error: 0.1191 - val_loss: 0.3289 - val_mean_squared_error: 0.2280 - val_mean_absolute_error: 0.3289\n",
      "Epoch 259/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1139 - mean_squared_error: 0.0251 - mean_absolute_error: 0.1139 - val_loss: 0.3212 - val_mean_squared_error: 0.2128 - val_mean_absolute_error: 0.3212\n",
      "Epoch 260/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1124 - mean_squared_error: 0.0249 - mean_absolute_error: 0.1124 - val_loss: 0.3164 - val_mean_squared_error: 0.2084 - val_mean_absolute_error: 0.3164\n",
      "Epoch 261/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1132 - mean_squared_error: 0.0250 - mean_absolute_error: 0.1132 - val_loss: 0.3104 - val_mean_squared_error: 0.2114 - val_mean_absolute_error: 0.3104- mean_absolute_e\n",
      "Epoch 262/1000\n",
      "31210/31210 [==============================] - 16s 508us/sample - loss: 0.1118 - mean_squared_error: 0.0242 - mean_absolute_error: 0.1118 - val_loss: 0.3249 - val_mean_squared_error: 0.2274 - val_mean_absolute_error: 0.3249\n",
      "Epoch 263/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1099 - mean_squared_error: 0.0235 - mean_absolute_error: 0.1099 - val_loss: 0.3091 - val_mean_squared_error: 0.2006 - val_mean_absolute_error: 0.3091\n",
      "Epoch 264/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1063 - mean_squared_error: 0.0222 - mean_absolute_error: 0.1063 - val_loss: 0.3380 - val_mean_squared_error: 0.2443 - val_mean_absolute_error: 0.3380\n",
      "Epoch 265/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1067 - mean_squared_error: 0.0225 - mean_absolute_error: 0.1067 - val_loss: 0.3147 - val_mean_squared_error: 0.2138 - val_mean_absolute_error: 0.3147\n",
      "Epoch 266/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1105 - mean_squared_error: 0.0238 - mean_absolute_error: 0.1105 - val_loss: 0.3003 - val_mean_squared_error: 0.2008 - val_mean_absolute_error: 0.3003\n",
      "Epoch 267/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1120 - mean_squared_error: 0.0243 - mean_absolute_error: 0.1120 - val_loss: 0.3161 - val_mean_squared_error: 0.2125 - val_mean_absolute_error: 0.3161\n",
      "Epoch 268/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1117 - mean_squared_error: 0.0241 - mean_absolute_error: 0.1117 - val_loss: 0.3169 - val_mean_squared_error: 0.2159 - val_mean_absolute_error: 0.3169\n",
      "Epoch 269/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1127 - mean_squared_error: 0.0245 - mean_absolute_error: 0.1127 - val_loss: 0.3259 - val_mean_squared_error: 0.2188 - val_mean_absolute_error: 0.3259\n",
      "Epoch 270/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1172 - mean_squared_error: 0.0260 - mean_absolute_error: 0.1172 - val_loss: 0.3438 - val_mean_squared_error: 0.2478 - val_mean_absolute_error: 0.3438\n",
      "Epoch 271/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1213 - mean_squared_error: 0.0279 - mean_absolute_error: 0.1213 - val_loss: 0.3072 - val_mean_squared_error: 0.2009 - val_mean_absolute_error: 0.3072\n",
      "Epoch 272/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1183 - mean_squared_error: 0.0267 - mean_absolute_error: 0.1183 - val_loss: 0.3078 - val_mean_squared_error: 0.2033 - val_mean_absolute_error: 0.3078 mean_squar - ETA: 4s - loss: 0.1199 - mean_squared_error: 0.0263 -\n",
      "Epoch 273/1000\n",
      "31210/31210 [==============================] - 16s 508us/sample - loss: 0.1134 - mean_squared_error: 0.0246 - mean_absolute_error: 0.1134 - val_loss: 0.3347 - val_mean_squared_error: 0.2333 - val_mean_absolute_error: 0.3347\n",
      "Epoch 274/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1110 - mean_squared_error: 0.0238 - mean_absolute_error: 0.1110 - val_loss: 0.3364 - val_mean_squared_error: 0.2307 - val_mean_absolute_error: 0.3364\n",
      "Epoch 275/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1126 - mean_squared_error: 0.0245 - mean_absolute_error: 0.1126 - val_loss: 0.3395 - val_mean_squared_error: 0.2374 - val_mean_absolute_error: 0.3395\n",
      "Epoch 276/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1149 - mean_squared_error: 0.0253 - mean_absolute_error: 0.1149 - val_loss: 0.3101 - val_mean_squared_error: 0.2047 - val_mean_absolute_error: 0.3101\n",
      "Epoch 277/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1116 - mean_squared_error: 0.0244 - mean_absolute_error: 0.1116 - val_loss: 0.3412 - val_mean_squared_error: 0.2516 - val_mean_absolute_error: 0.3412\n",
      "Epoch 278/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1140 - mean_squared_error: 0.0258 - mean_absolute_error: 0.1140 - val_loss: 0.3220 - val_mean_squared_error: 0.2226 - val_mean_absolute_error: 0.3220\n",
      "Epoch 279/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1091 - mean_squared_error: 0.0233 - mean_absolute_error: 0.1091 - val_loss: 0.3162 - val_mean_squared_error: 0.2094 - val_mean_absolute_error: 0.3162\n",
      "Epoch 280/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1088 - mean_squared_error: 0.0233 - mean_absolute_error: 0.1088 - val_loss: 0.3184 - val_mean_squared_error: 0.2201 - val_mean_absolute_error: 0.3184\n",
      "Epoch 281/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1106 - mean_squared_error: 0.0238 - mean_absolute_error: 0.1106 - val_loss: 0.3097 - val_mean_squared_error: 0.2102 - val_mean_absolute_error: 0.3097\n",
      "Epoch 282/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1113 - mean_squared_error: 0.0240 - mean_absolute_error: 0.1113 - val_loss: 0.3161 - val_mean_squared_error: 0.2152 - val_mean_absolute_error: 0.3161\n",
      "Epoch 283/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1098 - mean_squared_error: 0.0235 - mean_absolute_error: 0.1098 - val_loss: 0.3188 - val_mean_squared_error: 0.2119 - val_mean_absolute_error: 0.3188\n",
      "Epoch 284/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1067 - mean_squared_error: 0.0226 - mean_absolute_error: 0.1067 - val_loss: 0.3316 - val_mean_squared_error: 0.2328 - val_mean_absolute_error: 0.3316d_error: 0.0226 - mean_absolute_error: 0.10\n",
      "Epoch 285/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1085 - mean_squared_error: 0.0231 - mean_absolute_error: 0.1085 - val_loss: 0.3049 - val_mean_squared_error: 0.2046 - val_mean_absolute_error: 0.3049\n",
      "Epoch 286/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1085 - mean_squared_error: 0.0234 - mean_absolute_error: 0.1085 - val_loss: 0.3264 - val_mean_squared_error: 0.2283 - val_mean_absolute_error: 0.3264\n",
      "Epoch 287/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1076 - mean_squared_error: 0.0229 - mean_absolute_error: 0.1076 - val_loss: 0.3433 - val_mean_squared_error: 0.2432 - val_mean_absolute_error: 0.3433\n",
      "Epoch 288/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1100 - mean_squared_error: 0.0236 - mean_absolute_error: 0.1100 - val_loss: 0.3316 - val_mean_squared_error: 0.2248 - val_mean_absolute_error: 0.3316\n",
      "Epoch 289/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1133 - mean_squared_error: 0.0249 - mean_absolute_error: 0.1133 - val_loss: 0.3265 - val_mean_squared_error: 0.2230 - val_mean_absolute_error: 0.3265\n",
      "Epoch 290/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1163 - mean_squared_error: 0.0260 - mean_absolute_error: 0.1163 - val_loss: 0.3121 - val_mean_squared_error: 0.2141 - val_mean_absolute_error: 0.3121\n",
      "Epoch 291/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1125 - mean_squared_error: 0.0248 - mean_absolute_error: 0.1125 - val_loss: 0.3337 - val_mean_squared_error: 0.2396 - val_mean_absolute_error: 0.3337\n",
      "Epoch 292/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1106 - mean_squared_error: 0.0237 - mean_absolute_error: 0.1106 - val_loss: 0.3153 - val_mean_squared_error: 0.2043 - val_mean_absolute_error: 0.3153\n",
      "Epoch 293/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1063 - mean_squared_error: 0.0222 - mean_absolute_error: 0.1063 - val_loss: 0.3345 - val_mean_squared_error: 0.2378 - val_mean_absolute_error: 0.3345\n",
      "Epoch 294/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1079 - mean_squared_error: 0.0228 - mean_absolute_error: 0.1079 - val_loss: 0.3084 - val_mean_squared_error: 0.2101 - val_mean_absolute_error: 0.3084\n",
      "Epoch 295/1000\n",
      "31210/31210 [==============================] - 16s 508us/sample - loss: 0.1087 - mean_squared_error: 0.0232 - mean_absolute_error: 0.1087 - val_loss: 0.3270 - val_mean_squared_error: 0.2309 - val_mean_absolute_error: 0.3270\n",
      "Epoch 296/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1094 - mean_squared_error: 0.0234 - mean_absolute_error: 0.1094 - val_loss: 0.3408 - val_mean_squared_error: 0.2424 - val_mean_absolute_error: 0.3408\n",
      "Epoch 297/1000\n",
      "31210/31210 [==============================] - 16s 508us/sample - loss: 0.1091 - mean_squared_error: 0.0234 - mean_absolute_error: 0.1091 - val_loss: 0.3295 - val_mean_squared_error: 0.2274 - val_mean_absolute_error: 0.3295\n",
      "Epoch 298/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1093 - mean_squared_error: 0.0234 - mean_absolute_error: 0.1093 - val_loss: 0.3180 - val_mean_squared_error: 0.2211 - val_mean_absolute_error: 0.3180\n",
      "Epoch 299/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1124 - mean_squared_error: 0.0243 - mean_absolute_error: 0.1124 - val_loss: 0.3240 - val_mean_squared_error: 0.2240 - val_mean_absolute_error: 0.3240\n",
      "Epoch 300/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1090 - mean_squared_error: 0.0232 - mean_absolute_error: 0.1090 - val_loss: 0.3322 - val_mean_squared_error: 0.2364 - val_mean_absolute_error: 0.33220.1094 - mean_squared_error: 0.0235 - mean_\n",
      "Epoch 301/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1114 - mean_squared_error: 0.0240 - mean_absolute_error: 0.1114 - val_loss: 0.3114 - val_mean_squared_error: 0.2016 - val_mean_absolute_error: 0.3114\n",
      "Epoch 302/1000\n",
      "31210/31210 [==============================] - 16s 508us/sample - loss: 0.1100 - mean_squared_error: 0.0237 - mean_absolute_error: 0.1100 - val_loss: 0.3205 - val_mean_squared_error: 0.2208 - val_mean_absolute_error: 0.3205squared_error: 0.0239 - mean_absolu\n",
      "Epoch 303/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1066 - mean_squared_error: 0.0224 - mean_absolute_error: 0.1066 - val_loss: 0.3086 - val_mean_squared_error: 0.2083 - val_mean_absolute_error: 0.3086\n",
      "Epoch 304/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1073 - mean_squared_error: 0.0226 - mean_absolute_error: 0.1073 - val_loss: 0.3258 - val_mean_squared_error: 0.2212 - val_mean_absolute_error: 0.3258\n",
      "Epoch 305/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1078 - mean_squared_error: 0.0227 - mean_absolute_error: 0.1078 - val_loss: 0.3477 - val_mean_squared_error: 0.2469 - val_mean_absolute_error: 0.3477\n",
      "Epoch 306/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1067 - mean_squared_error: 0.0224 - mean_absolute_error: 0.1067 - val_loss: 0.3350 - val_mean_squared_error: 0.2377 - val_mean_absolute_error: 0.3350\n",
      "Epoch 307/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1070 - mean_squared_error: 0.0224 - mean_absolute_error: 0.1070 - val_loss: 0.3153 - val_mean_squared_error: 0.2174 - val_mean_absolute_error: 0.3153\n",
      "Epoch 308/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1073 - mean_squared_error: 0.0226 - mean_absolute_error: 0.1073 - val_loss: 0.3055 - val_mean_squared_error: 0.2028 - val_mean_absolute_error: 0.3055\n",
      "Epoch 309/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1071 - mean_squared_error: 0.0225 - mean_absolute_error: 0.1071 - val_loss: 0.3064 - val_mean_squared_error: 0.2062 - val_mean_absolute_error: 0.3064\n",
      "Epoch 310/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1085 - mean_squared_error: 0.0229 - mean_absolute_error: 0.1085 - val_loss: 0.3078 - val_mean_squared_error: 0.2082 - val_mean_absolute_error: 0.3078\n",
      "Epoch 311/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1123 - mean_squared_error: 0.0244 - mean_absolute_error: 0.1123 - val_loss: 0.3084 - val_mean_squared_error: 0.2039 - val_mean_absolute_error: 0.30846s - loss: 0.1134 - mean_squ - ETA: 4s - loss: 0.1153 - mean_squared - ETA: 2s - loss: 0.1138 \n",
      "Epoch 312/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1091 - mean_squared_error: 0.0231 - mean_absolute_error: 0.1091 - val_loss: 0.2968 - val_mean_squared_error: 0.1898 - val_mean_absolute_error: 0.2968\n",
      "Epoch 313/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1123 - mean_squared_error: 0.0244 - mean_absolute_error: 0.1123 - val_loss: 0.3143 - val_mean_squared_error: 0.2145 - val_mean_absolute_error: 0.3143\n",
      "Epoch 314/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1115 - mean_squared_error: 0.0240 - mean_absolute_error: 0.1115 - val_loss: 0.3293 - val_mean_squared_error: 0.2330 - val_mean_absolute_error: 0.3293\n",
      "Epoch 315/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1170 - mean_squared_error: 0.0262 - mean_absolute_error: 0.1170 - val_loss: 0.3278 - val_mean_squared_error: 0.2167 - val_mean_absolute_error: 0.3278mean_absolute_error: 0. - ETA: 2s - loss: 0.115\n",
      "Epoch 316/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1101 - mean_squared_error: 0.0236 - mean_absolute_error: 0.1101 - val_loss: 0.3340 - val_mean_squared_error: 0.2336 - val_mean_absolute_error: 0.3340\n",
      "Epoch 317/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1081 - mean_squared_error: 0.0230 - mean_absolute_error: 0.1081 - val_loss: 0.3228 - val_mean_squared_error: 0.2219 - val_mean_absolute_error: 0.3228\n",
      "Epoch 318/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1082 - mean_squared_error: 0.0229 - mean_absolute_error: 0.1082 - val_loss: 0.3243 - val_mean_squared_error: 0.2265 - val_mean_absolute_error: 0.3243\n",
      "Epoch 319/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1104 - mean_squared_error: 0.0237 - mean_absolute_error: 0.1104 - val_loss: 0.3231 - val_mean_squared_error: 0.2260 - val_mean_absolute_error: 0.3231\n",
      "Epoch 320/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1114 - mean_squared_error: 0.0242 - mean_absolute_error: 0.1114 - val_loss: 0.3243 - val_mean_squared_error: 0.2237 - val_mean_absolute_error: 0.3243\n",
      "Epoch 321/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1096 - mean_squared_error: 0.0235 - mean_absolute_error: 0.1096 - val_loss: 0.3169 - val_mean_squared_error: 0.2163 - val_mean_absolute_error: 0.3169\n",
      "Epoch 322/1000\n",
      "31210/31210 [==============================] - 16s 507us/sample - loss: 0.1089 - mean_squared_error: 0.0231 - mean_absolute_error: 0.1089 - val_loss: 0.3185 - val_mean_squared_error: 0.2207 - val_mean_absolute_error: 0.3185\n",
      "Epoch 323/1000\n",
      "31210/31210 [==============================] - 16s 512us/sample - loss: 0.1082 - mean_squared_error: 0.0231 - mean_absolute_error: 0.1082 - val_loss: 0.3302 - val_mean_squared_error: 0.2274 - val_mean_absolute_error: 0.3302\n",
      "Epoch 324/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1094 - mean_squared_error: 0.0234 - mean_absolute_error: 0.1094 - val_loss: 0.3282 - val_mean_squared_error: 0.2236 - val_mean_absolute_error: 0.3282\n",
      "Epoch 325/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1146 - mean_squared_error: 0.0254 - mean_absolute_error: 0.1146 - val_loss: 0.3268 - val_mean_squared_error: 0.2214 - val_mean_absolute_error: 0.3268\n",
      "Epoch 326/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1171 - mean_squared_error: 0.0265 - mean_absolute_error: 0.1171 - val_loss: 0.3247 - val_mean_squared_error: 0.2205 - val_mean_absolute_error: 0.3247bsolute_error: 0.11\n",
      "Epoch 327/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1141 - mean_squared_error: 0.0251 - mean_absolute_error: 0.1141 - val_loss: 0.3272 - val_mean_squared_error: 0.2327 - val_mean_absolute_error: 0.3272\n",
      "Epoch 328/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1105 - mean_squared_error: 0.0236 - mean_absolute_error: 0.1105 - val_loss: 0.3380 - val_mean_squared_error: 0.2330 - val_mean_absolute_error: 0.3380\n",
      "Epoch 329/1000\n",
      "31210/31210 [==============================] - 16s 506us/sample - loss: 0.1080 - mean_squared_error: 0.0228 - mean_absolute_error: 0.1080 - val_loss: 0.3494 - val_mean_squared_error: 0.2567 - val_mean_absolute_error: 0.3494\n",
      "Epoch 330/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1031 - mean_squared_error: 0.0211 - mean_absolute_error: 0.1031 - val_loss: 0.3108 - val_mean_squared_error: 0.2103 - val_mean_absolute_error: 0.3108\n",
      "Epoch 331/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1038 - mean_squared_error: 0.0215 - mean_absolute_error: 0.1038 - val_loss: 0.2935 - val_mean_squared_error: 0.1927 - val_mean_absolute_error: 0.2935\n",
      "Epoch 332/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1054 - mean_squared_error: 0.0220 - mean_absolute_error: 0.1054 - val_loss: 0.3080 - val_mean_squared_error: 0.2041 - val_mean_absolute_error: 0.3080\n",
      "Epoch 333/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1078 - mean_squared_error: 0.0228 - mean_absolute_error: 0.1078 - val_loss: 0.3173 - val_mean_squared_error: 0.2152 - val_mean_absolute_error: 0.3173\n",
      "Epoch 334/1000\n",
      "31210/31210 [==============================] - 16s 505us/sample - loss: 0.1100 - mean_squared_error: 0.0235 - mean_absolute_error: 0.1100 - val_loss: 0.3367 - val_mean_squared_error: 0.2397 - val_mean_absolute_error: 0.3367\n",
      "Wall time: 1h 28min 12s\n"
     ]
    }
   ],
   "source": [
    "    %%time\n",
    "    hist = model.fit(trX, trY, epochs=epochs, batch_size=batch_size, shuffle=False, validation_data=(vaX, vaY), callbacks=[history, early_stopping])  # , checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPredict = model.predict(trX, batch_size=batch_size)\n",
    "validPredict = model.predict(vaX, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Training Score > MSE ==  0.05285718749382127  MAE ==  0.18010500578783262\n",
      "Error Validation Score > MSE ==  0.23965152241528467  MAE ==  0.3367353684909806\n"
     ]
    }
   ],
   "source": [
    "trPredict = trainPredict.reshape([-1])\n",
    "trainY = trY.reshape([-1])\n",
    "valPredict = validPredict.reshape([-1])\n",
    "validY = vaY.reshape([-1])\n",
    "\n",
    "print('Error Training Score > MSE == ', (np.mean(np.square(trainY-trPredict))), ' MAE == ', mean_absolute_error(trainY, trPredict))\n",
    "print('Error Validation Score > MSE == ', (np.mean(np.square(validY-valPredict))), ' MAE == ', mean_absolute_error(validY, valPredict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 120, 13)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teX = X[-1,:,:].reshape(-1,120,13)\n",
    "teX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredict = model.predict(teX)\n",
    "testPredict = testPredict.reshape(24,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 12)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trash = norm_df[-24:, 1:]\n",
    "trash.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 13)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YY = np.concatenate([testPredict, trash], axis=1)\n",
    "YY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Or=min_max_scaler.inverse_transform(YY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "testval = Or[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1204.62053199, 1146.90532943, 1095.14701709, 1060.1261208 ,\n",
       "       1032.36619281, 1004.66263214,  998.79329175, 1016.37066672,\n",
       "       1005.77483412, 1096.80285743, 1237.04640952, 1307.90922486,\n",
       "       1369.47986469, 1398.68184783, 1389.63403449, 1401.17292098,\n",
       "       1398.06301845, 1373.22127262, 1320.13875226, 1235.47732317,\n",
       "       1257.74612657, 1299.50744992, 1271.96801998, 1240.27383334])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"3. test_Panama.txt\", \"r\")\n",
    "realy = file.readlines()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1032.263\\n',\n",
       " '1017.5806\\n',\n",
       " '1000.2797\\n',\n",
       " '987.2383\\n',\n",
       " '970.3667\\n',\n",
       " '957.6767\\n',\n",
       " '922.3729\\n',\n",
       " '924.4692\\n',\n",
       " '954.3947\\n',\n",
       " '986.0648\\n',\n",
       " '989.1948\\n',\n",
       " '1005.6986\\n',\n",
       " '1006.6785\\n',\n",
       " '999.7711\\n',\n",
       " '1000.8847\\n',\n",
       " '1002.8859\\n',\n",
       " '993.8271\\n',\n",
       " '988.1441\\n',\n",
       " '1086.8084\\n',\n",
       " '1126.1699\\n',\n",
       " '1130.5479\\n',\n",
       " '1123.8589\\n',\n",
       " '1072.263\\n',\n",
       " '1076.9789']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "realy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"3. test_Panama.txt\", \"r\") as file:\n",
    "    realy2 = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1032.263\\n',\n",
       " '1017.5806\\n',\n",
       " '1000.2797\\n',\n",
       " '987.2383\\n',\n",
       " '970.3667\\n',\n",
       " '957.6767\\n',\n",
       " '922.3729\\n',\n",
       " '924.4692\\n',\n",
       " '954.3947\\n',\n",
       " '986.0648\\n',\n",
       " '989.1948\\n',\n",
       " '1005.6986\\n',\n",
       " '1006.6785\\n',\n",
       " '999.7711\\n',\n",
       " '1000.8847\\n',\n",
       " '1002.8859\\n',\n",
       " '993.8271\\n',\n",
       " '988.1441\\n',\n",
       " '1086.8084\\n',\n",
       " '1126.1699\\n',\n",
       " '1130.5479\\n',\n",
       " '1123.8589\\n',\n",
       " '1072.263\\n',\n",
       " '1076.9789']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "realy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"3. test_Panama.txt\", \"r\")\n",
    "realy4 = np.array(1, dtype=np.float32)\n",
    "while True:\n",
    "    realy3 = file.readline()\n",
    "    if not realy3:\n",
    "        break\n",
    "    realy4 = np.append(realy4,float(realy3.strip()))\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1032.263 , 1017.5806, 1000.2797,  987.2383,  970.3667,  957.6767,\n",
       "        922.3729,  924.4692,  954.3947,  986.0648,  989.1948, 1005.6986,\n",
       "       1006.6785,  999.7711, 1000.8847, 1002.8859,  993.8271,  988.1441,\n",
       "       1086.8084, 1126.1699, 1130.5479, 1123.8589, 1072.263 , 1076.9789])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "realy4 = realy4[1:]\n",
    "realy4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Test Score > MSE ==  55871.714633049814  MAE ==  200.22796554125748\n"
     ]
    }
   ],
   "source": [
    "print('Error Test Score > MSE == ', (np.mean(np.square(realy4-testval))), ' MAE == ', mean_absolute_error(realy4, testval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 13)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YYr = np.concatenate([realy4.reshape(-1,1), trash], axis=1)\n",
    "YYr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "YYr2 = min_max_scaler.fit_transform(YYr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "testy = YYr2[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30109646,  0.04720492, -0.25196633, -0.47748131, -0.769229  ,\n",
       "       -0.98866749, -1.59914915, -1.56289943, -1.04542061, -0.49777374,\n",
       "       -0.44364904, -0.15826143, -0.14131677, -0.26076116, -0.24150453,\n",
       "       -0.2068993 , -0.36354622, -0.461818  ,  1.24430842,  1.92495679,\n",
       "        2.0006622 ,  1.88499443,  0.99278592,  1.07433438])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testy = testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tePredict = testPredict.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Test Score > MSE ==  0.8854171240232519  MAE ==  0.7644576295528296\n"
     ]
    }
   ],
   "source": [
    "print('Error Test Score > MSE == ', (np.mean(np.square(testy-tePredict))), ' MAE == ', mean_absolute_error(testy, tePredict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
